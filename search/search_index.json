{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-eessi-project-documentation","title":"Welcome to the EESSI project documentation!","text":"<p>Quote</p> <p>What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance?</p> <p>The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community.</p> <p>The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure.</p> <p>More details about the project are available in the different subsections:</p> <ul> <li>Project overview</li> <li>Filesystem layer</li> <li>Compatibility layer</li> <li>Software layer</li> <li>Pilot repository</li> <li>Getting access to EESSI</li> <li>Using EESSI</li> <li>Software testing</li> <li>Getting support</li> <li>Meetings</li> <li>Project partners</li> <li>Contact info</li> </ul> <p>The EESSI project was presented at the 6th EasyBuild User Meeting in January 2021, check the recording:</p>"},{"location":"bot/","title":"Build-test-deploy bot","text":"<p>Building, testing, and deploying software is done by one or more bot instances.</p> <p>The EESSI build-test-deploy bot  is implemented as a GitHub App in the <code>eessi-bot-software-layer</code> repository.</p> <p>It operates in the context of pull requests to the <code>compatibility-layer</code> repository or the <code>software-layer</code> repository, and follows the instructions supplied by humans, so the procedure of adding software to EESSI is semi-automatic.</p> <p>It leverages the scripts provided in the <code>bot/</code> subdirectory of the target repository (see for example here), like <code>bot/build.sh</code> to build software, and <code>bot/check-result.sh</code> to check whether the software was built correctly.</p>"},{"location":"bot/#high-level-design","title":"High-level design","text":"<p>The bot  consists of two components: the event handler, and the job manager.</p>"},{"location":"bot/#event-handler","title":"Event handler","text":"<p>The bot event handler is responsible for handling GitHub events for the GitHub repositories it is registered to.</p> <p>It is triggered for every event that it receives from GitHub. Most events are ignored, but specific events trigger the bot to take action.</p> <p>Examples of actionable events are submitting of a comment that starts with <code>bot:</code>, which may specify an instruction for the bot like building software, or adding a <code>bot:deploy</code> label (see deploying).</p>"},{"location":"bot/#job-manager","title":"Job manager","text":"<p>The bot job manager is responsible for monitoring the queued and running jobs, and reporting back when jobs completed.</p> <p>It runs every couple of minutes as a cron job.</p>"},{"location":"bot/#basics","title":"Basics","text":"<p>Instructions for the bot  should always start with <code>bot:</code>.</p> <p>To get help from the bot, post a comment with <code>bot: help</code>.</p> <p>To make the bot report how it is configured, post a comment with <code>bot: show_config</code>.</p>"},{"location":"bot/#permissions","title":"Permissions","text":"<p>The bot  is configured to only act on instructions issued by specific GitHub accounts.</p> <p>There are separate configuration options for allowing to send instructions to the bot, to trigger building of software, and to deploy software installations in to the EESSI repository.</p> <p>Note</p> <p>Ask for help in the <code>#software-layer-bot</code> channel of the EESSI Slack if needed!</p>"},{"location":"bot/#building","title":"Building","text":"<p>To instruct the bot  to build software, one or more <code>build</code> instructions should be issued by posting a comment in the pull request (see also here).</p> <p>The most basic build instruction that can be sent to the bot is:</p> <pre><code>bot: build\n</code></pre> <p>Warning</p> <p>Only use <code>bot: build</code> if you are confident that it is OK to do so.</p> <p>Most likely, you want to supply one or more filters to avoid that the bot builds for all its configurations.</p>"},{"location":"bot/#filters","title":"Filters","text":"<p>Build instructions can include filters that are applied by each bot instance to determine which builds should be executed, based on:</p> <ul> <li><code>instance</code>: the <code>name</code> of the bot instance, for example <code>instance:aws</code> for the bot instance running in AWS;</li> <li><code>repository</code>: the target repository, for example <code>eessi-2023.06-software</code> which corresponds to the 2023.06 version of the EESSI software layer;</li> <li><code>architecture</code>: the name of the CPU microarchitecture, for example <code>x86_64/amd/zen2</code>;</li> </ul> <p>Note</p> <p>Use <code>:</code> as separator to specify a value for a particular filter, do not add spaces after the <code>:</code>.</p> <p>The bot recognizes shorthands for the supported filters, so you can use <code>inst:...</code> instead of <code>instance:...</code>, <code>repo:...</code> instead of <code>repository:...</code>, and <code>arch:...</code> instead of <code>architecture:...</code>.</p>"},{"location":"bot/#combining-filters","title":"Combining filters","text":"<p>You can combine multiple filters in a single <code>build</code> instruction. Separate filters with a space, order of filters does not matter.</p> <p>For example:</p> <pre><code>bot: build repo:eessi-2023.06-software arch:x86_64/amd/zen2\n</code></pre>"},{"location":"bot/#multiple-build-instructions","title":"Multiple build instructions","text":"<p>You can issue multiple build instructions in a single comment, even across multiple bot instances, repositories, and CPU targets. Specify one build instruction per line.</p> <p>For example:</p> <pre><code>bot: build repo:eessi-2023.06-software arch:x86_64/amd/zen3 inst:aws\nbot: build repo:eessi-2023.06-software arch:aarch64/generic inst:azure\n</code></pre> <p>Note</p> <p>The bot applies the filters with partial matching, which you can use to combine multiple build instructions into a single one.</p> <p>For example, if you only want to build for all <code>aarch64</code> CPU targets, you can use <code>arch:aarch64</code> as filter.</p> <p>The same applies to the <code>instance</code> and <code>repository</code> filters.</p>"},{"location":"bot/#behind-the-scenes","title":"Behind-the-scenes","text":""},{"location":"bot/#processing-build-instructions","title":"Processing build instructions","text":"<p>When the bot receives build instructions through a comment in a pull request, they are processed by the event handler component. It will:</p> <p>1) Combine its active configuration (instance name, repositories, supported CPU targets)    and the build instructions to prepare a list of jobs to submit;</p> <p>2) Create a working directory for each job, including a Slurm job script that    runs the <code>bot/build.sh</code> script in the context of the changes proposed in the pull request to build the    software, and runs <code>bot/check-result.sh</code> script at the end to check whether the build was successful;</p> <p>3) Submit each prepared job to a workernode that can build for the specified CPU target, and put a hold on it.</p>"},{"location":"bot/#managing-build-jobs","title":"Managing build jobs","text":"<p>During the next iteration of the job manager, the submitted jobs are released and queued for execution.</p> <p>The job manager also monitors the running jobs at regular intervals, and reports back in the pull request when a job has completed. It also reports the result (<code>SUCCESS</code>  or <code>FAILURE</code> ), based on the result of the <code>bot/check-result.sh</code> script.</p>"},{"location":"bot/#artefacts","title":"Artefacts","text":"<p>If all goes well, each job should produce a tarball as an artefact, which contains the software installations and the corresponding environment module files.</p> <p>The message reported by the job manager provides an overview of the contents of the artefact, which was created by the <code>bot/check-result.sh</code> script.</p>"},{"location":"bot/#testing","title":"Testing","text":"<p>Warning</p> <p>The test phase is not implemented yet in the bot.</p> <p>We intend to use the EESSI test suite in different OS configurations to verify that the software that was built works as expected.</p>"},{"location":"bot/#deploying","title":"Deploying","text":"<p>To deploy the artefacts that were obtained in the build phase, you should add the <code>bot: deploy</code> label to the pull request.</p> <p>This will trigger the event handler to upload the artefacts for ingestion into the EESSI repository.</p>"},{"location":"bot/#behind-the-scenes_1","title":"Behind-the-scenes","text":"<p>The current setup for the software-layer repository, is as follows:</p> <ul> <li>The bot deploys the artefacts (tarballs) to an S3 bucket in AWS, along with a metadata file, using the   <code>eessi-upload-to-staging</code> script;</li> <li>A cron job that runs every couple of minutes on the CernVM-FS Stratum-0 server opens a pull request to   the (private) EESSI/staging repository, to move the metadata file for   each uploaded tarball from the <code>staged</code> to the <code>approved</code> directory;</li> <li>Once that pull request gets merged, the target is automatically ingested into the EESSI repository by a cron job   on the Stratum-0 server, and the metadata file is moved from <code>approved</code> to <code>ingested</code> in the <code>EESSI/staging</code> repository;</li> </ul>"},{"location":"compatibility_layer/","title":"Compatibility layer","text":"<p>The middle layer of the EESSI project is the compatibility layer, which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL).</p> <p>For this we rely on Gentoo Prefix, by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage.</p> <p>The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.</p>"},{"location":"contact/","title":"Contact info","text":"<p>For more information:</p> <ul> <li>visit our website https://www.eessi-hpc.org</li> <li>consult our documentation at https://eessi.github.io</li> <li>reach out to one of the project partners</li> <li>check out our GitHub repositories at https://github.com/EESSI</li> <li>follow us on Twitter: https://twitter.com/eessi_hpc</li> </ul> <p>A Slack channel is available for the EESSI community (an invitation is required to join).</p> <p></p>"},{"location":"filesystem_layer/","title":"Filesystem layer","text":"<p>The bottom layer of the EESSI project is the filesystem layer, which is responsible for distributing the software stack.</p> <p>For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way.</p> <p>CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers.</p> <p></p> <p>The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module.</p> <p>For a (basic) introduction to CernVM-FS, see this presentation.</p> <p>Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .</p>"},{"location":"meetings/","title":"Meetings","text":""},{"location":"meetings/#monthly-meetings-online","title":"Monthly meetings (online)","text":"<p>Online EESSI update meeting, every 1st Thursday of the month at 14:00 CE(S)T.</p> <p>More info via https://github.com/EESSI/meetings/wiki</p>"},{"location":"meetings/#physical-meetings","title":"Physical meetings","text":"<ul> <li>EESSI Community Meeting in Amsterdam (NL), 14-16 Sept 2022</li> </ul>"},{"location":"meetings/#physical-meetings-archive","title":"Physical meetings (archive)","text":""},{"location":"meetings/#2019","title":"2019","text":"<ul> <li>Meeting in Cambridge (UK), 20-21 May 2019</li> </ul>"},{"location":"meetings/#2020","title":"2020","text":"<ul> <li>Meeting in Groningen (NL), 16 Jan 2020</li> <li>Meeting in Delft (NL), 5 Mar 2020</li> </ul>"},{"location":"overview/","title":"Overview of the EESSI project","text":""},{"location":"overview/#scope-goals","title":"Scope &amp; Goals","text":"<p>Through the EESSI project, we want to set up a shared stack of scientific software installations, and by doing so avoid a lot of duplicate work across HPC sites.</p> <p>For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use.</p> <p>Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL, and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V).</p> <p>Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently.</p>"},{"location":"overview/#inspiration","title":"Inspiration","text":"<p>The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones.</p> <p>The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\".</p> <p>It has also been presented at the 5th EasyBuild User Meetings (slides, recorded talk), and is well documented.</p>"},{"location":"overview/#layered-structure","title":"Layered structure","text":"<p>The EESSI project consists of 3 layers.</p> <p></p> <p>The bottom layer is the filesystem layer, which is responsible for distributing the software stack across clients.</p> <p>The middle layer is a compatibility layer, which ensures that the software stack is compatible with multiple different client operating systems.</p> <p>The top layer is the software layer, which contains the actual scientific software applications and their dependencies.</p> <p>The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on.</p>"},{"location":"overview/#opportunities","title":"Opportunities","text":"<p>We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers.</p> <p>Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere.</p> <p>We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems.</p> <p>By working together with the developers of scientific software we can provide vetted installations for the broad HPC community.</p>"},{"location":"overview/#challenges","title":"Challenges","text":"<p>There are many challenges in an ambitious project like this, including (but probably not limited to):</p> <ul> <li>Finding time and manpower to get the software stack set up properly;</li> <li>Leveraging system sources like network interconnect (MPI &amp; co), accelerators (GPUs), ...;</li> <li>Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ...</li> <li>Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...;</li> <li>Integration with resource managers (Slurm) and vendor provided software (Cray PE);</li> <li>Convincing HPC site admins to adopt EESSI;</li> </ul>"},{"location":"overview/#current-status","title":"Current status","text":"<p>(June 2020)</p> <p>We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward.</p> <p>Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed.</p>"},{"location":"partners/","title":"Project partners","text":""},{"location":"partners/#delft-university-of-technology-the-netherlands","title":"Delft University of Technology (The Netherlands)","text":"<ul> <li>Robbert Eggermont</li> <li>Koen Mulderij</li> </ul>"},{"location":"partners/#dell-technologies-europe","title":"Dell Technologies (Europe)","text":"<ul> <li>Walther Blom, High Education &amp; Research</li> <li>Jaco van Dijk, Higher Education</li> </ul>"},{"location":"partners/#eindhoven-university-of-technology","title":"Eindhoven University of Technology","text":"<ul> <li>Patrick Van Brakel</li> </ul>"},{"location":"partners/#ghent-university-belgium","title":"Ghent University (Belgium)","text":"<ul> <li>Kenneth Hoste, HPC-UGent</li> </ul>"},{"location":"partners/#hpcnow-spain","title":"HPCNow! (Spain)","text":"<ul> <li>Oriol Mula Valls</li> </ul>"},{"location":"partners/#julich-supercomputing-centre-germany","title":"J\u00fclich Supercomputing Centre (Germany)","text":"<ul> <li>Alan O'Cais</li> </ul>"},{"location":"partners/#university-of-cambridge-united-kingdom","title":"University of Cambridge (United Kingdom)","text":"<ul> <li>Mark Sharpley, Research Computing Services Division</li> </ul>"},{"location":"partners/#university-of-groningen-the-netherlands","title":"University of Groningen (The Netherlands)","text":"<ul> <li>Bob Dr\u00f6ge, Center for Information Technology</li> <li>Henk-Jan Zilverberg, Center for Information Technology</li> </ul>"},{"location":"partners/#university-of-twente-the-netherlands","title":"University of Twente (The Netherlands)","text":"<ul> <li>Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS)</li> </ul>"},{"location":"partners/#university-of-oslo-norway","title":"University of Oslo (Norway)","text":"<ul> <li>Terje Kvernes</li> </ul>"},{"location":"partners/#university-of-bergen-norway","title":"University of Bergen (Norway)","text":"<ul> <li>Thomas R\u00f6blitz</li> </ul>"},{"location":"partners/#vrije-universiteit-amsterdam-the-netherlands","title":"Vrije Universiteit Amsterdam (The Netherlands)","text":"<ul> <li>Peter Stol</li> </ul>"},{"location":"partners/#surf-the-netherlands","title":"SURF (The Netherlands)","text":"<ul> <li>Caspar van Leeuwen</li> <li>Marco Verdicchio</li> <li>Bas van der Vlies</li> </ul>"},{"location":"pilot/","title":"Pilot repository","text":""},{"location":"pilot/#pilot-software-stack-202112","title":"Pilot software stack (2021.12)","text":""},{"location":"pilot/#caveats","title":"Caveats","text":"<p>The current EESSI pilot software stack (version 2021.12) is the 7th iteration, and there are some known issues and limitations, please take these into account:</p> <ul> <li>First of all: the EESSI pilot software stack is NOT READY FOR PRODUCTION!</li> </ul> <p>Do not use it for production work, and be careful when testing it on production systems!</p>"},{"location":"pilot/#reporting-problems","title":"Reporting problems","text":"<p>If you notice any problems, please report them via https://github.com/EESSI/software-layer/issues.</p>"},{"location":"pilot/#accessing-the-eessi-pilot-repository-through-singularity","title":"Accessing the EESSI pilot repository through Singularity","text":"<p>The easiest way to access the EESSI pilot repository is by using Singularity. If Singularity is installed already, no admin privileges are required. No other software is needed either on the host.</p> <p>A container image is available in the GitHub Container Registry  (see https://github.com/EESSI/filesystem-layer/pkgs/container/client-pilot). It only contains a minimal operating system + the necessary packages to access the EESSI pilot repository through CernVM-FS, and it is suitable for <code>aarch64</code>, <code>ppc64le</code>, and <code>x86_64</code>.</p> <p>The container image can be used directly by Singularity (no prior download required), as follows:</p> <ul> <li> <p>First, create some local directories in <code>/tmp/$USER</code> which will be bind mounted in the container:   <pre><code>mkdir -p /tmp/$USER/{var-lib-cvmfs,var-run-cvmfs,home}\n</code></pre>   These provides space for the CernVM-FS cache, and an empty home directory to use in the container.</p> </li> <li> <p>Set the <code>$SINGULARITY_BIND</code> and <code>$SINGULARITY_HOME</code> environment variables to configure Singularity:   <pre><code>export SINGULARITY_BIND=\"/tmp/$USER/var-run-cvmfs:/var/run/cvmfs,/tmp/$USER/var-lib-cvmfs:/var/lib/cvmfs\"\nexport SINGULARITY_HOME=\"/tmp/$USER/home:/home/$USER\"\n</code></pre></p> </li> <li> <p>Start the container using <code>singularity shell</code>, using <code>--fusemount</code> to mount the EESSI pilot repository   (using the <code>cvmfs2</code> command that is included in the container image):   <pre><code>export EESSI_PILOT=\"container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org\"\nsingularity shell --fusemount \"$EESSI_PILOT\" docker://ghcr.io/eessi/client-pilot:centos7\n</code></pre></p> </li> <li> <p>This should give you a shell in the container, where the EESSI pilot repository is mounted:    <pre><code>$ singularity shell --fusemount \"$EESSI_PILOT\" docker://ghcr.io/eessi/client-pilot:centos7\nINFO:    Using cached SIF image\nCernVM-FS: pre-mounted on file descriptor 3\nCernVM-FS: loading Fuse module... done\nSingularity&gt;\n</code></pre></p> </li> <li>It is possible that you see some scary looking warnings, but those can be ignored for now.</li> </ul> <p>To verify that things are working, check the contents of the <code>/cvmfs/pilot.eessi-hpc.org/versions/2021.12</code> directory:    <pre><code>Singularity&gt; ls /cvmfs/pilot.eessi-hpc.org/versions/2021.12\ncompat  init  software\n</code></pre></p>"},{"location":"pilot/#standard-installation","title":"Standard installation","text":"<p>For those with privileges on their system, there are a number of example installation scripts for different architectures and operating systems available in the EESSI demo repository.</p> <p>Here we prefer the Singularity approach as we can guarantee that the container image is up to date.</p>"},{"location":"pilot/#setting-up-the-eessi-environment","title":"Setting up the EESSI environment","text":"<p>Once you have the EESSI pilot repository mounted, you can set up the environment by sourcing the provided init script:</p> <pre><code>source /cvmfs/pilot.eessi-hpc.org/versions/2021.12/init/bash\n</code></pre> <p>If all goes well, you should see output like this:</p> <pre><code>Found EESSI pilot repo @ /cvmfs/pilot.eessi-hpc.org/versions/2021.12!\nUsing x86_64/intel/haswell as software subdirectory.\nUsing /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI pilot software stack, have fun!\n[EESSI pilot 2021.12] $ </code></pre> <p>Now you're all set up! Go ahead and explore the software stack using \"<code>module avail</code>\", and go wild with testing the available software installations!</p>"},{"location":"pilot/#testing-the-eessi-pilot-software-stack","title":"Testing the EESSI pilot software stack","text":"<p>Please test the EESSI pilot software stack as you see fit: running simple commands, performing small calculations or running small benchmarks, etc.</p> <p>Test scripts that have been verified to work correctly using the pilot software stack are available at https://github.com/EESSI/software-layer/tree/main/tests .</p>"},{"location":"pilot/#giving-feedback-or-reporting-problems","title":"Giving feedback or reporting problems","text":"<p>Any feedback is welcome, and questions or problems reports are welcome as well, through one of the EESSI communication channels:</p> <ul> <li>(preferred!) EESSI <code>software-layer</code> GitHub repository: https://github.com/EESSI/software-layer/issues</li> <li>EESSI mailing list (<code>eessi@list.rug.nl</code>)</li> <li>EESSI Slack: https://eessi-hpc.slack.com (get an invite via https://www.eessi-hpc.org/join)</li> <li>monthly EESSI meetings (first Thursday of the month at 2pm CEST)</li> </ul>"},{"location":"pilot/#available-software","title":"Available software","text":"<p>(last update: Mar 21st 2022)</p> <p>EESSI currently supports the following HPC applications as well as all their dependencies:</p> <ul> <li>GROMACS (2020.1 and 2020.4)</li> <li>OpenFOAM (v2006 and 8)</li> <li>R (4.0.0) + R-bundle-Bioconductor (3.11) + RStudio Server (1.3.1093)</li> <li>TensorFlow (2.3.1) and Horovod (0.21.3)</li> <li>OSU-Micro-Benchmarks (5.6.3)</li> <li>ReFrame (3.9.1)</li> <li>Spark (3.1.1)</li> <li>IPython (7.15.0)</li> <li>QuantumESPRESSO (6.6) (currently not available on <code>ppc64le</code>)</li> <li>WRF (3.9.1.1)</li> </ul> <pre><code>[EESSI pilot 2021.12] $ module --nx avail\n\n--------------------------- /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all ----------------------------\n   ant/1.10.8-Java-11                                              LMDB/0.9.24-GCCcore-9.3.0\n   Arrow/0.17.1-foss-2020a-Python-3.8.2                            lz4/1.9.2-GCCcore-9.3.0\n   Bazel/3.6.0-GCCcore-9.3.0                                       Mako/1.1.2-GCCcore-9.3.0\n   Bison/3.5.3-GCCcore-9.3.0                                       MariaDB-connector-c/3.1.7-GCCcore-9.3.0\n   Boost/1.72.0-gompi-2020a                                        matplotlib/3.2.1-foss-2020a-Python-3.8.2\n   cairo/1.16.0-GCCcore-9.3.0                                      Mesa/20.0.2-GCCcore-9.3.0\n   CGAL/4.14.3-gompi-2020a-Python-3.8.2                            Meson/0.55.1-GCCcore-9.3.0-Python-3.8.2\n   CMake/3.16.4-GCCcore-9.3.0                                      METIS/5.1.0-GCCcore-9.3.0\n   CMake/3.20.1-GCCcore-10.3.0                                     MPFR/4.0.2-GCCcore-9.3.0\n   code-server/3.7.3                                               NASM/2.14.02-GCCcore-9.3.0\n   DB/18.1.32-GCCcore-9.3.0                                        ncdf4/1.17-foss-2020a-R-4.0.0\n   DB/18.1.40-GCCcore-10.3.0                                       netCDF-Fortran/4.5.2-gompi-2020a\n   double-conversion/3.1.5-GCCcore-9.3.0                           netCDF/4.7.4-gompi-2020a\n   Doxygen/1.8.17-GCCcore-9.3.0                                    nettle/3.6-GCCcore-9.3.0\n   EasyBuild/4.5.0                                                 networkx/2.4-foss-2020a-Python-3.8.2\n   EasyBuild/4.5.1                                         (D)     Ninja/1.10.0-GCCcore-9.3.0\n   Eigen/3.3.7-GCCcore-9.3.0                                       NLopt/2.6.1-GCCcore-9.3.0\n   Eigen/3.3.9-GCCcore-10.3.0                                      NSPR/4.25-GCCcore-9.3.0\n   ELPA/2019.11.001-foss-2020a                                     NSS/3.51-GCCcore-9.3.0\n   expat/2.2.9-GCCcore-9.3.0                                       nsync/1.24.0-GCCcore-9.3.0\n   expat/2.2.9-GCCcore-10.3.0                                      numactl/2.0.13-GCCcore-9.3.0\n   FFmpeg/4.2.2-GCCcore-9.3.0                                      numactl/2.0.14-GCCcore-10.3.0\n   FFTW/3.3.8-gompi-2020a                                          OpenBLAS/0.3.9-GCC-9.3.0\n   FFTW/3.3.9-gompi-2021a                                          OpenBLAS/0.3.15-GCC-10.3.0\n   flatbuffers/1.12.0-GCCcore-9.3.0                                OpenFOAM/v2006-foss-2020a\n   FlexiBLAS/3.0.4-GCC-10.3.0                                      OpenFOAM/8-foss-2020a                              (D)\n   fontconfig/2.13.92-GCCcore-9.3.0                                OpenMPI/4.0.3-GCC-9.3.0\n   foss/2020a                                                      OpenMPI/4.1.1-GCC-10.3.0\n   foss/2021a                                                      OpenPGM/5.2.122-GCCcore-9.3.0\n   freetype/2.10.1-GCCcore-9.3.0                                   OpenSSL/1.1                                        (D)\n   FriBidi/1.0.9-GCCcore-9.3.0                                     OSU-Micro-Benchmarks/5.6.3-gompi-2020a\n   GCC/9.3.0                                                       Pango/1.44.7-GCCcore-9.3.0\n   GCC/10.3.0                                                      ParaView/5.8.0-foss-2020a-Python-3.8.2-mpi\n   GCCcore/9.3.0                                                   PCRE/8.44-GCCcore-9.3.0\n   GCCcore/10.3.0                                                  PCRE2/10.34-GCCcore-9.3.0\n   Ghostscript/9.52-GCCcore-9.3.0                                  Perl/5.30.2-GCCcore-9.3.0\n   giflib/5.2.1-GCCcore-9.3.0                                      Perl/5.32.1-GCCcore-10.3.0\n   git/2.23.0-GCCcore-9.3.0-nodocs                                 pixman/0.38.4-GCCcore-9.3.0\n   git/2.32.0-GCCcore-10.3.0-nodocs                        (D)     pkg-config/0.29.2-GCCcore-9.3.0\n   GLib/2.64.1-GCCcore-9.3.0                                       pkg-config/0.29.2-GCCcore-10.3.0\n   GLPK/4.65-GCCcore-9.3.0                                         pkg-config/0.29.2                                  (D)\n   GMP/6.2.0-GCCcore-9.3.0                                         pkgconfig/1.5.1-GCCcore-9.3.0-Python-3.8.2\n   GMP/6.2.1-GCCcore-10.3.0                                        PMIx/3.1.5-GCCcore-9.3.0\n   gnuplot/5.2.8-GCCcore-9.3.0                                     PMIx/3.2.3-GCCcore-10.3.0\n   GObject-Introspection/1.64.0-GCCcore-9.3.0-Python-3.8.2         poetry/1.0.9-GCCcore-9.3.0-Python-3.8.2\n   gompi/2020a                                                     protobuf-python/3.13.0-foss-2020a-Python-3.8.2\n   gompi/2021a                                                     protobuf/3.13.0-GCCcore-9.3.0\n   groff/1.22.4-GCCcore-9.3.0                                      pybind11/2.4.3-GCCcore-9.3.0-Python-3.8.2\n   groff/1.22.4-GCCcore-10.3.0                                     pybind11/2.6.2-GCCcore-10.3.0\n   GROMACS/2020.1-foss-2020a-Python-3.8.2                          Python/2.7.18-GCCcore-9.3.0\n   GROMACS/2020.4-foss-2020a-Python-3.8.2                  (D)     Python/3.8.2-GCCcore-9.3.0\n   GSL/2.6-GCC-9.3.0                                               Python/3.9.5-GCCcore-10.3.0-bare\n   gzip/1.10-GCCcore-9.3.0                                         Python/3.9.5-GCCcore-10.3.0\n   h5py/2.10.0-foss-2020a-Python-3.8.2                             PyYAML/5.3-GCCcore-9.3.0\n   HarfBuzz/2.6.4-GCCcore-9.3.0                                    Qt5/5.14.1-GCCcore-9.3.0\n   HDF5/1.10.6-gompi-2020a                                         QuantumESPRESSO/6.6-foss-2020a\n   Horovod/0.21.3-foss-2020a-TensorFlow-2.3.1-Python-3.8.2         R-bundle-Bioconductor/3.11-foss-2020a-R-4.0.0\n   hwloc/2.2.0-GCCcore-9.3.0                                       R/4.0.0-foss-2020a\n   hwloc/2.4.1-GCCcore-10.3.0                                      re2c/1.3-GCCcore-9.3.0\n   hypothesis/6.13.1-GCCcore-10.3.0                                RStudio-Server/1.3.1093-foss-2020a-Java-11-R-4.0.0\n   ICU/66.1-GCCcore-9.3.0                                          Rust/1.52.1-GCCcore-10.3.0\n   ImageMagick/7.0.10-1-GCCcore-9.3.0                              ScaLAPACK/2.1.0-gompi-2020a\n   IPython/7.15.0-foss-2020a-Python-3.8.2                          ScaLAPACK/2.1.0-gompi-2021a-fb\n   JasPer/2.0.14-GCCcore-9.3.0                                     scikit-build/0.10.0-foss-2020a-Python-3.8.2\n   Java/11.0.2                                             (11)    SciPy-bundle/2020.03-foss-2020a-Python-3.8.2\n   jbigkit/2.1-GCCcore-9.3.0                                       SciPy-bundle/2021.05-foss-2021a\n   JsonCpp/1.9.4-GCCcore-9.3.0                                     SCOTCH/6.0.9-gompi-2020a\n   LAME/3.100-GCCcore-9.3.0                                        snappy/1.1.8-GCCcore-9.3.0\n   libarchive/3.5.1-GCCcore-10.3.0                                 Spark/3.1.1-foss-2020a-Python-3.8.2\n   libcerf/1.13-GCCcore-9.3.0                                      SQLite/3.31.1-GCCcore-9.3.0\n   libdrm/2.4.100-GCCcore-9.3.0                                    SQLite/3.35.4-GCCcore-10.3.0\n   libevent/2.1.11-GCCcore-9.3.0                                   SWIG/4.0.1-GCCcore-9.3.0\n   libevent/2.1.12-GCCcore-10.3.0                                  Szip/2.1.1-GCCcore-9.3.0\n   libfabric/1.11.0-GCCcore-9.3.0                                  Tcl/8.6.10-GCCcore-9.3.0\n   libfabric/1.12.1-GCCcore-10.3.0                                 Tcl/8.6.11-GCCcore-10.3.0\n   libffi/3.3-GCCcore-9.3.0                                        tcsh/6.22.02-GCCcore-9.3.0\n   libffi/3.3-GCCcore-10.3.0                                       TensorFlow/2.3.1-foss-2020a-Python-3.8.2\n   libgd/2.3.0-GCCcore-9.3.0                                       time/1.9-GCCcore-9.3.0\n   libGLU/9.0.1-GCCcore-9.3.0                                      Tk/8.6.10-GCCcore-9.3.0\n   libglvnd/1.2.0-GCCcore-9.3.0                                    Tkinter/3.8.2-GCCcore-9.3.0\n   libiconv/1.16-GCCcore-9.3.0                                     UCX/1.8.0-GCCcore-9.3.0\n   libjpeg-turbo/2.0.4-GCCcore-9.3.0                               UCX/1.10.0-GCCcore-10.3.0\n   libpciaccess/0.16-GCCcore-9.3.0                                 UDUNITS/2.2.26-foss-2020a\n   libpciaccess/0.16-GCCcore-10.3.0                                UnZip/6.0-GCCcore-9.3.0\n   libpng/1.6.37-GCCcore-9.3.0                                     UnZip/6.0-GCCcore-10.3.0\n   libsndfile/1.0.28-GCCcore-9.3.0                                 WRF/3.9.1.1-foss-2020a-dmpar\n   libsodium/1.0.18-GCCcore-9.3.0                                  X11/20200222-GCCcore-9.3.0\n   LibTIFF/4.1.0-GCCcore-9.3.0                                     x264/20191217-GCCcore-9.3.0\n   libtirpc/1.2.6-GCCcore-9.3.0                                    x265/3.3-GCCcore-9.3.0\n   libunwind/1.3.1-GCCcore-9.3.0                                   xorg-macros/1.19.2-GCCcore-9.3.0\n   libxc/4.3.4-GCC-9.3.0                                           xorg-macros/1.19.3-GCCcore-10.3.0\n   libxml2/2.9.10-GCCcore-9.3.0                                    Xvfb/1.20.9-GCCcore-9.3.0\n   libxml2/2.9.10-GCCcore-10.3.0                                   Yasm/1.3.0-GCCcore-9.3.0\n   libyaml/0.2.2-GCCcore-9.3.0                                     ZeroMQ/4.3.2-GCCcore-9.3.0\n   LittleCMS/2.9-GCCcore-9.3.0                                     Zip/3.0-GCCcore-9.3.0\n   LLVM/9.0.1-GCCcore-9.3.0                                        zstd/1.4.4-GCCcore-9.3.0\n</code></pre>"},{"location":"pilot/#architecture-and-micro-architecture-support","title":"Architecture and micro-architecture support","text":""},{"location":"pilot/#x86_64","title":"x86_64","text":"<ul> <li>generic (currently implies <code>march=x86-64</code> and <code>-mtune=generic</code>)</li> <li>AMD<ul> <li>zen2 (Rome)</li> <li>zen3 (Milan)</li> </ul> </li> <li>Intel<ul> <li>haswell</li> <li>skylake_avx512</li> </ul> </li> </ul>"},{"location":"pilot/#aarch64arm64","title":"aarch64/arm64","text":"<ul> <li>generic (currently implies <code>-march=armv8-a</code> and <code>-mtune=generic</code>)</li> <li>AWS Graviton2</li> </ul>"},{"location":"pilot/#ppc64le","title":"ppc64le","text":"<ul> <li>generic</li> <li>power9le</li> </ul>"},{"location":"pilot/#easybuild-configuration","title":"EasyBuild configuration","text":"<p>EasyBuild v4.5.1 was used to install the software in the <code>2021.12</code> version of the pilot repository. For some installations pull requests with changes that will be included in later EasyBuild versions were leveraged, see the build script that was used.</p> <p>An example configuration of the build environment based on https://github.com/EESSI/software-layer can be seen here: <pre><code>$ eb --show-config\n#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath         (E) = /tmp/eessi-build/easybuild/build\ncontainerpath     (E) = /tmp/eessi-build/easybuild/containers\ndebug             (E) = True\nfilter-deps       (E) = Autoconf, Automake, Autotools, binutils, bzip2, cURL, DBus, flex, gettext, gperf, help2man, intltool, libreadline, libtool, Lua, M4, makeinfo, ncurses, util-linux, XZ, zlib\nfilter-env-vars   (E) = LD_LIBRARY_PATH\nhooks             (E) = /home/eessi-build/software-layer/eb_hooks.py\nignore-osdeps     (E) = True\ninstallpath       (E) = /cvmfs/pilot.eessi-hpc.org/2021.06/software/linux/x86_64/intel/haswell\nmodule-extensions (E) = True\npackagepath       (E) = /tmp/eessi-build/easybuild/packages\nprefix            (E) = /tmp/eessi-build/easybuild\nrepositorypath    (E) = /tmp/eessi-build/easybuild/ebfiles_repo\nrobot-paths       (D) = /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/software/EasyBuild/4.5.1/easybuild/easyconfigs\nrpath             (E) = True\nsourcepath        (E) = /tmp/eessi-build/easybuild/sources:\nsysroot           (E) = /cvmfs/pilot.eessi-hpc.org/versions/2021.12/compat/linux/x86_64\ntrace             (E) = True\nzip-logs          (E) = bzip2\n</code></pre></p>"},{"location":"software_layer/","title":"Software layer","text":"<p>The top layer of the EESSI project is the software layer, which provides the actual scientific software installations.</p> <p>To install the software we include in our stack, we use EasyBuild, a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation).</p> <p>To access these software installation we provide environment module files and use Lmod, a modern environment modules tool which has been widely adopted in the HPC community in recent years.</p> <p>We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture.</p> <p>The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.</p>"},{"location":"software_testing/","title":"Software testing","text":"<p>WARNING: development of the software test suite has only just started and is a work in progress. This page describes how the test suite will be designed, but many things are not implemented yet and the design may still change.</p>"},{"location":"software_testing/#description-of-the-software-test-suite","title":"Description of the software test suite","text":""},{"location":"software_testing/#framework","title":"Framework","text":"<p>The EESSI project uses the ReFrame framework for software testing. ReFrame is designed particularly for testing HPC software and thus has well integrated support for interacting with schedulers, as well as various launchers for MPI programs.</p>"},{"location":"software_testing/#test-variants","title":"Test variants","text":"<p>The EESSI software stack can be used in various ways, e.g. by using the container or when the CVMFS software stack is mounted natively. This means the commands that need to be run to test an application are different in both cases. Similarly, systems may have different hardware (CPUs v.s. GPUs, system size, etc). Thus, tests - e.g. a GROMACS test - may have different variants: one designed to run on CPUs, one on GPUs, one designed to run through the container, etc.</p> <p>The main goal of the EESSI test suite is to test the software stack on systems that have the EESSI CVMFS mounted natively. Some tests may also have variants that can run the same test through the container, but note that this setup is technically much more difficult. Thus, the main focus is on tests that run with a native CVMFS mount of the EESSI stack.</p> <p>By default, ReFrame runs all test variants it find. Thus, in our test suite, we prespecify a number of tags that can be used to select an appropriate subset of tests for your system. We recognize the following tags:</p> <ul> <li>container: tests that use the EESSI container to run the software. E.g. one variant of our GROMACS test uses <code>singularity exec</code> to launch the EESSI container, load the GROMACS module, and run the GROMACS test. </li> <li><code>native</code>: tests that rely on the EESSI software stack being available through the modules system. E.g. one variant of the GROMACS test loads the GROMACS module and runs the GROMACS test.</li> <li><code>singlecore</code>: tests designed to run on a single core</li> <li><code>singlenode</code>: tests designed to run on a single (multicore) node (note: may still use MPI for multiprocessing)</li> <li><code>small</code>: tests designed to run on 2-8 nodes.</li> <li><code>large</code>: tests designed to run on &gt;9 nodes.</li> <li><code>cpu</code>: test designed to run on CPU.</li> <li><code>gpu</code>, gpu_nvidia, gpu_amd: test designed to run on GPUs / nvidia GPUs / AMD GPUs.</li> </ul>"},{"location":"software_testing/#how-to-run-the-test-suite","title":"How to run the test suite","text":""},{"location":"software_testing/#general-requirements","title":"General requirements","text":"<ul> <li>A copy of the <code>tests</code> directory from software repository</li> </ul>"},{"location":"software_testing/#requirements-for-container-based-tests","title":"Requirements for container-based tests","text":"<p>Specifically for container-based tests, there are some requirements on the host system:</p> <ul> <li>An installation of ReFrame</li> <li>An MPI installation (to launch MPI tests) or PMIx-based launcher (e.g. SLURM compiled with PMIx support)</li> <li>Singularity</li> </ul> <p>The container based tests will use a so-called shared alien CVMFS cache to store temporary data. In addition, they use a local CVMFS cache for speed. For this reason, the container tests need to be pointed to one directory that is shared between nodes on your system, and one directory that is node-specific (preferably a local disk). The <code>shared_alien_cache_minimal.sh</code> script that is part of the test suite defines these, and sets up the correct CVMFS configuration. You will have to adapt the <code>SHAREDSPACE</code> and <code>LOCALSPACE</code> variables in that script for your system, and point them to a shared and node-local directory.</p>"},{"location":"software_testing/#setting-up-a-reframe-configuration-file","title":"Setting up a ReFrame configuration file","text":"<p>Once the prerequisites have been met, you'll need to create a ReFrame configuration file that matches your system (see the ReFrame documentation). If you want to use the container-based tests, you have to define a partition programming environment called <code>container</code> and make sure it loads any modules needed to provide the MPI installation and singularity command. For an example configuration file, check the <code>tests/reframe/config/settings.py</code> in the software-layer repository. Other than (potential) adaptations to the <code>container</code> environment, you should only really need to change the <code>systems</code> part.</p>"},{"location":"software_testing/#adapting-the-tests-to-your-system","title":"Adapting the tests to your system","text":"<p>For now, you will have to adapt the number of tasks specified in full-node tests to match the number of cores your machine has in a single node (in the future, you should be able to do this through the reframe configuration file). To do so, change all <code>self.num_tasks_per_node</code> you find in the various tests to that core count (unless they are 1, in which case the test specifically intended for only 1 process per node).</p>"},{"location":"software_testing/#an-example-run","title":"An example run","text":"<p>In this example, we assume your current directory is the <code>tests/reframe</code> folder. To list e.g. all single node, cpu-based application tests on a system that has the EESSI software environment available natively, you execute: <pre><code>reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu\n</code></pre> (assuming you adapted the config file in <code>config/settings.py</code> for your system). This should list the tests that are selected based on the provided tags. To run the tests, change the <code>-l</code> argument into a <code>-r</code>: <pre><code>reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu --performance-report\n</code></pre> To run the same tests with using the EESSI container, run: <pre><code>reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t container -t single -t cpu --performance-report\n</code></pre> Note that not all tests necessarily have implementations to run using the EESSI container: the primary focus of the test suite is for HPC sites to check the performance of their software suite. Such sites should have CVMFS mounted natively for optimal performance anyway.</p>"},{"location":"support/","title":"Getting support for EESSI","text":"<p>Thanks to the MultiXscale EuroHPC project we are able to provide support to the users of EESSI. </p> <p>The EESSI support portal is hosted in GitLab: https://gitlab.com/eessi/support.</p>"},{"location":"support/#open-issue","title":"How to report a problem or ask a question","text":"<p>We recommend you to use a GitLab account if you want to get help from the EESSI support team.</p> <p>If you have a GitLab account you can submit your problems or questions on  EESSI via the issue tracker of the EESSI support portal at https://gitlab.com/eessi/support/-/issues. Please use one of the provided templates (report a problem, software request, question, ...) when creating an issue.</p> <p>You can also contact us via our e-mail address <code>support (@) eessi.io</code>, which will automatically create a (private) issue in the EESSI support portal. When you send us an email, please provide us with as much information as possible on your question or problem. You can find an overview of the information that we would like to receive in the README of the EESSI support portal.</p>"},{"location":"support/#level-of-support","title":"Level of Support","text":"<p>We provide support for EESSI according to a \"reasonable effort\" standard. That means we will go into reasonable effort to help you, but we may not have the time to explore every potential cause, and it may not lead to a (quick) solution. You can compare this to the level of support you typically get from other active open source projects.</p> <p>Note that the more complete your reported issue is (e.g. description of the error, what you ran, the software environment in which you ran, minimal reproducer, etc.) the bigger the chance is that we can help you with \"reasonable effort\".</p>"},{"location":"support/#what-do-we-provide-support-for","title":"What do we provide support for","text":""},{"location":"support/#accessing-and-using-the-eessi-software-stack","title":"Accessing and using the EESSI software stack","text":"<p>If you have trouble connecting to the software stack, such as trouble related to installing or configuring CernVM-FS to access the EESSI filesystem layer, or running the software installations included in the EESSI compatibility layer or software layer, please contact us.</p> <p>Note that we can only help with problems related to the software installations (getting the software to run, to perform as expected, etc.). We do not provide support for using specific features of the provided software, nor can we fix (known or unknown) bugs in the software included in EESSI. We can only help with diagnosing and fixing problems that are caused by how the software was built and installed in EESSI.</p>"},{"location":"support/#software-requests","title":"Software requests","text":"<p>We are open to software requests for software that is not included in EESSI yet.</p> <p>The quickest way to add additional software to EESSI is by contributing it yourself as a community contribution, please see the documentation on adding software.</p> <p>Alternatively, you can send in a request to our support team. Please try to provide as much information on the software as possible: preferably use the issue template (which requires you to log in to GitLab), or make sure to cover the items listed here.</p> <p>Be aware that we can only provide software that has an appropriate open source license.</p>"},{"location":"support/#eessi-test-suite","title":"EESSI test suite","text":"<p>If you are using the EESSI test suite, you can get help via the EESSI support portal.</p>"},{"location":"support/#build-and-deploy-bot","title":"Build-and-deploy bot","text":"<p>If you are using the EESSI build-and-deploy bot, you can get help via the EESSI support portal.</p>"},{"location":"support/#what-do-we-not-provide-support-for","title":"What do we not provide support for","text":"<p>Do not contact the EESSI support team to get help with using software that is included in EESSI, unless you think the problems you are seeing are related to how the software was built and installed.</p> <p>Please consult the documentation of the software you are using, or contact the developers of the software directly, if you have questions regarding using the software, or if you think you have found a bug.</p>"},{"location":"filesystem_layer/stratum1/","title":"Setting up a Stratum 1","text":"<p>Setting up a Stratum 1 involves the following steps:</p> <ul> <li>set up the Stratum 1, preferably by running the Ansible playbook that we provide;</li> <li>request a Stratum 0 firewall exception for your Stratum 1 server;</li> <li>request a <code>&lt;your site&gt;.stratum1.cvmfs.eessi-infra.org</code> DNS entry;</li> <li>open a pull request to include the URL to your Stratum 1 in the EESSI configuration.</li> </ul> <p>The last two steps can be skipped if you want to host a \"private\" Stratum 1 for your site.</p>"},{"location":"filesystem_layer/stratum1/#requirements-for-a-stratum-1","title":"Requirements for a Stratum 1","text":"<p>The main requirements for a Stratum 1 server are a good network connection to the clients it is going to serve, and sufficient disk space. For the EESSI pilot, a few hundred gigabytes should suffice, but for production environments at least 1 TB would be recommended.</p> <p>In terms of cores and memory, a machine with just a few (~4) cores and 4-8 GB of memory should suffice.</p> <p>Various Linux distributions are supported, but we recommend one based on RHEL 7 or 8.</p> <p>Finally, make sure that ports 80 (for the Apache web server) and 8000 are open.</p>"},{"location":"filesystem_layer/stratum1/#step-1-set-up-the-stratum-1","title":"Step 1: set up the Stratum 1","text":"<p>The recommended way for setting up an EESSI Stratum 1 is by running the Ansible playbook <code>stratum1.yml</code> from the filesystem-layer repository on GitHub.</p> <p>Installing a Stratum 1 requires a GEO API license key, which will be used to find the (geographically) closest Stratum 1 server for your client and proxies. More information on how to (freely) obtain this key is available in the CVMFS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-replica.html#geo-api-setup.</p> <p>You can put your license key in the local configuration file <code>inventory/local_site_specific_vars.yml</code>.</p> <p>Furthermore, the Stratum 1 runs a Squid server. The template configuration file can be found at <code>templates/eessi_stratum1_squid.conf.j2</code>. If you want to customize it, for instance for limiting the access to the Stratum 1, you can make your own version of this template file  and point to it by setting <code>local_stratum1_cvmfs_squid_conf_src</code> in <code>inventory/local_site_specific_vars.yml</code>. See the comments in the example file for more details.</p> <p>Start by installing Ansible:</p> <pre><code>sudo yum install -y ansible\n</code></pre> <p>Then install Ansible roles for EESSI:</p> <pre><code>ansible-galaxy role install -r requirements.yml -p ./roles --force\n</code></pre> <p>Make sure you have enough space in <code>/srv</code> (on the Stratum 1) since the snapshot of the Stratum 0 will end up there by default. To alter the directory where the snapshot gets copied to you can add this variable in <code>inventory/host_vars/&lt;url-or-ip-to-your-stratum1&gt;</code>:</p> <pre><code>cvmfs_srv_mount: /srv\n</code></pre> <p>Make sure that you have added the hostname or IP address of your server to the <code>inventory/hosts</code> file. Finally, install the Stratum 1 using one of the two following options.</p> <p>Option 1:</p> <pre><code># -b to run as root, optionally use -K if a sudo password is required\nansible-playbook -b [-K] -e @inventory/local_site_specific_vars.yml stratum1.yml\n</code></pre> <p>Option2:</p> <p>Create a ssh key pair and make sure the <code>ansible-host-keys.pub</code> is in the <code>$HOME/.ssh/authorized_keys</code> file on your Stratum 1 server.</p> <pre><code>ssh-keygen -b 2048 -t rsa -f ~/.ssh/ansible-host-keys -q -N \"\"\n</code></pre> <p>Then run the playbook:</p> <pre><code>ansible-playbook -b --private-key ~/.ssh/ansible-host-keys -e @inventory/local_site_specific_vars.yml stratum1.yml\n</code></pre> <p>Running the playbook will automatically make replicas of all the repositories defined in <code>group_vars/all.yml</code>.</p>"},{"location":"filesystem_layer/stratum1/#step-2-request-a-firewall-exception","title":"Step 2: request a firewall exception","text":"<p>(This step is not implemented yet and can be skipped)</p> <p>You can request a firewall exception rule to be added for your Stratum 1 server by opening an issue on the GitHub page of the filesystem layer repository.</p> <p>Make sure to include the IP address of your server.</p>"},{"location":"filesystem_layer/stratum1/#step-3-verification-of-the-stratum-1","title":"Step 3: Verification of the Stratum 1","text":"<p>When the playbook has finished your Stratum 1 should be ready. In order to test your Stratum 1, even without a client installed, you can use <code>curl</code>.</p> <p><pre><code>curl --head http://&lt;url-or-ip-to-your-stratum1&gt;/cvmfs/pilot.eessi-hpc.org/.cvmfspublished\n</code></pre> This should return:</p> <pre><code>HTTP/1.1 200 OK\n...\nX-Cache: MISS from &lt;url-or-ip-to-your-stratum1&gt;\n</code></pre> <p>The second time you run it, you should get a cache hit:</p> <pre><code>X-Cache: HIT from &lt;url-or-ip-to-your-stratum1&gt;\n</code></pre> <p>Example with the Norwegian Stratum 1:</p> <pre><code>curl --head http://bgo-no.stratum1.cvmfs.eessi-infra.org/cvmfs/pilot.eessi-hpc.org/.cvmfspublished\n</code></pre> <p>You can also test access to your Stratum 1 from a client, for which you will have to install the CVMFS client. </p> <p>Then run the following command to add your newly created Stratum 1 to the existing list of EESSI Stratum 1 servers by creating a local CVMFS configuration file:</p> <pre><code>echo 'CVMFS_SERVER_URL=\"http://&lt;url-or-ip-to-your-stratum1&gt;/cvmfs/@fqrn@;$CVMFS_SERVER_URL\"' | sudo tee -a /etc/cvmfs/domain.d/eessi-hpc.org.local\n</code></pre> <p>If this is the first time you set up the client you now run:</p> <pre><code>sudo cvmfs_config setup\n</code></pre> <p>If you already had configured the client before, you can simply reload the config:</p> <pre><code>sudo cvmfs_config reload -c pilot.eessi-hpc.org\n</code></pre> <p>Finally, verify that the client connects to your new Stratum 1 by running:</p> <pre><code>cvmfs_config stat -v pilot.eessi-hpc.org\n</code></pre> <p>Assuming that your new Stratum 1 is the geographically closest one to your client, this should return:</p> <pre><code>Connection: http://&lt;url-or-ip-to-your-stratum1&gt;/cvmfs/pilot.eessi-hpc.org through proxy DIRECT (online)\n</code></pre>"},{"location":"filesystem_layer/stratum1/#step-4-request-an-eessi-dns-name","title":"Step 4: request an EESSI DNS name","text":"<p>In order to keep the configuration clean and easy, all the EESSI Stratum 1 servers have a DNS name <code>&lt;your site&gt;.stratum1.cvmfs.eessi-infra.org</code>, where <code>&lt;your site&gt;</code> is often a short name or abbreviation followed by the country code (e.g. <code>rug-nl</code> or <code>bgo-no</code>).  You can request this for your Stratum 1 by mentioning this in the issue that you created in Step 2, or by opening another issue.</p>"},{"location":"filesystem_layer/stratum1/#step-5-include-your-stratum-1-in-the-eessi-configuration","title":"Step 5: include your Stratum 1 in the EESSI configuration","text":"<p>If you want to include your Stratum 1 in the EESSI configuration, i.e. allow any (nearby) client to be able to use it, you can open a pull request with updated configuration files. You will only have to add the URL to your Stratum 1 to the  <code>urls</code> list of the <code>eessi_cvmfs_server_urls</code> variable in the <code>all.yml</code> file.</p>"},{"location":"getting_access/eessi_container/","title":"EESSI container","text":"<p>The <code>eessi_container.sh</code> script provides a very easy yet versatile means to access EESSI.</p> <p>This page guides you through several example scenarios illustrating the use of the script.</p>"},{"location":"getting_access/eessi_container/#prerequisites","title":"Prerequisites","text":"<ul> <li>Apptainer 1.0.0 (or newer), or Singularity 3.7.x<ul> <li>Check with <code>apptainer --version</code> or <code>singularity --version</code></li> <li>Support for the <code>--fusemount</code> option in the <code>shell</code> and <code>run</code> subcommands is required</li> </ul> </li> <li>Git<ul> <li>Check with <code>git --version</code></li> </ul> </li> </ul>"},{"location":"getting_access/eessi_container/#preparation","title":"Preparation","text":"<p>Clone the <code>EESSI/software-layer</code> repository and change into the <code>software-layer</code> directory by running these commands:</p> <pre><code>git clone https://github.com/EESSI/software-layer.git\ncd software-layer\n</code></pre>"},{"location":"getting_access/eessi_container/#quickstart","title":"Quickstart","text":"<p>Run the <code>eessi_container</code> script (from the <code>software-layer</code> directory) to start a shell session in the EESSI container:</p> <pre><code>./eessi_container.sh\n</code></pre> <p>Note</p> <p>Startup will take a bit longer the first time you run this because the container image is downloaded and converted.</p> <p>You should see output like <pre><code>Using /tmp/eessi.abc123defg as tmp storage (add '--resume /tmp/eessi.abc123defg' to resume where this session ended).\nPulling container image from docker://ghcr.io/eessi/build-node:debian11 to /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nLaunching container with command (next line):\nsingularity -q shell --fusemount container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nCernVM-FS: pre-mounted on file descriptor 3\nApptainer&gt; CernVM-FS: loading Fuse module... done\nfuse: failed to clone device fd: Inappropriate ioctl for device\nfuse: trying to continue without -o clone_fd.\n\nApptainer&gt;\n</code></pre></p> <p>Note</p> <p>You may have to press enter to clearly see the prompt as some messages beginning with <code>CernVM-FS:</code> have been printed after the first prompt <code>Apptainer&gt;</code> was shown.</p> <p>To start using EESSI, see Using EESSI/Setting up your environment.</p>"},{"location":"getting_access/eessi_container/#help-for-eessi_containersh","title":"Help for <code>eessi_container.sh</code>","text":"<p>The example in the Quickstart section facilitates an interactive session with read access to the EESSI pilot software stack. It does not require any command line options, because the script <code>eessi_container.sh</code> uses some carefully chosen defaults. To view all options of the script and its default values, run the command <pre><code>./eessi_container.sh --help\n</code></pre> You should see the following output <pre><code>usage: ./eessi_container.sh [OPTIONS] [[--] SCRIPT or COMMAND]\n OPTIONS:\n  -a | --access {ro,rw}  - ro (read-only), rw (read &amp; write) [default: ro]\n  -c | --container IMG   - image file or URL defining the container to use\n                           [default: docker://ghcr.io/eessi/build-node:debian11]\n  -h | --help            - display this usage information [default: false]\n  -g | --storage DIR     - directory space on host machine (used for\n                           temporary data) [default: 1. TMPDIR, 2. /tmp]\n  -l | --list-repos      - list available repository identifiers [default: false]\n  -m | --mode MODE       - with MODE==shell (launch interactive shell) or\n                           MODE==run (run a script or command) [default: shell]\n  -r | --repository CFG  - configuration file or identifier defining the\n                           repository to use [default: EESSI-pilot via\n                           container configuration]\n  -u | --resume DIR/TGZ  - resume a previous run from a directory or tarball,\n                           where DIR points to a previously used tmp directory\n                           (check for output 'Using DIR as tmp ...' of a previous\n                           run) and TGZ is the path to a tarball which is\n                           unpacked the tmp dir stored on the local storage space\n                           (see option --storage above) [default: not set]\n  -s | --save DIR/TGZ    - save contents of tmp directory to a tarball in\n                           directory DIR or provided with the fixed full path TGZ\n                           when a directory is provided, the format of the\n                           tarball's name will be {REPO_ID}-{TIMESTAMP}.tgz\n                           [default: not set]\n  -v | --verbose         - display more information [default: false]\n  -x | --http-proxy URL  - provides URL for the env variable http_proxy\n                           [default: not set]; uses env var $http_proxy if set\n  -y | --https-proxy URL - provides URL for the env variable https_proxy\n                           [default: not set]; uses env var $https_proxy if set\n\n If value for --mode is 'run', the SCRIPT/COMMAND provided is executed. If\n arguments to the script/command start with '-' or '--', use the flag terminator\n '--' to let eessi_container.sh stop parsing arguments.\n</code></pre></p> <p>So, the defaults are equal to running the command <pre><code>./eessi_container.sh --access ro --container docker://ghcr.io/eessi/build-node:debian11 --mode shell --repository EESSI-pilot\n</code></pre> and it would either create a temporary directory under <code>${TMPDIR}</code> (if defined), or <code>/tmp</code> (if <code>${TMPDIR}</code> is not defined).</p> <p>The remainder of this page will demonstrate different scenarios using some of the command line options used for read-only access.</p> <p>Other options supported by the script will be discussed in a yet-to-be written section covering building software to be added to the EESSI stack.</p>"},{"location":"getting_access/eessi_container/#resuming-a-previous-session","title":"Resuming a previous session","text":"<p>You may have noted the following line in the output of <code>eessi_container.sh</code> <pre><code>Using /tmp/eessi.abc123defg as tmp storage (add '--resume /tmp/eessi.abc123defg' to resume where this session ended).\n</code></pre></p> <p>Note</p> <p>The parameter after <code>--resume</code> (<code>/tmp/eessi.abc123defg</code>) will be different when you run <code>eessi_container.sh</code>.</p> <p>Scroll back in your terminal and copy it so you can pass it to <code>--resume</code>.</p> <p>Try the following command to \"resume\" from the last session. <pre><code>./eessi_container.sh --resume /tmp/eessi.abc123defg\n</code></pre> This should run much faster because the container image has been cached in the temporary directory (<code>/tmp/eessi.abc123defg</code>). You should get to the prompt (<code>Apptainer&gt;</code> or <code>Singularity&gt;</code>) and can use EESSI with the state where you left the previous session.</p> <p>Note</p> <p>The state refers to what was stored on disk, not what was changed in memory. Particularly, any environment (variable) settings are not restored automatically.</p> <p>Because the <code>/tmp/eessi.abc123defg</code> directory contains a <code>home</code> directory which includes the saved history of your last session, you can easily restore the environment (variable) settings. Type <code>history</code> to see which commands you ran. You should be able to access the history as you would do in a normal terminal session.</p>"},{"location":"getting_access/eessi_container/#running-a-simple-command","title":"Running a simple command","text":"<p>Let's \"<code>ls /cvmfs/pilot.eessi-hpc.org</code>\" through the <code>eessi_container.sh</code> script to check if the CernVM-FS EESSI pilot repository is accessible:</p> <pre><code>./eessi_container.sh --mode run ls /cvmfs/pilot.eessi-hpc.org\n</code></pre> <p>You should see an output such as</p> <pre><code>Using /tmp/eessi.abc123defg as tmp storage (add '--resume /tmp/eessi.abc123defg' to resume where this session ended).$\nPulling container image from docker://ghcr.io/eessi/build-node:debian11 to /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nLaunching container with command (next line):\nsingularity -q run --fusemount container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif ls /cvmfs/pilot.eessi-hpc.org\nCernVM-FS: pre-mounted on file descriptor 3\nCernVM-FS: loading Fuse module... done\nhost_injections  latest  versions\n</code></pre> <p>Note that this time no interactive shell session is started in the container: only the provided command is run in the container, and when that finishes you are back in the shell session where you ran the <code>eessi_container.sh</code> script.</p> <p>This is because we used the <code>--mode run</code> command line option.</p> <p>Note</p> <p>The last line in the output is the output of the <code>ls</code> command, which shows the contents of the <code>/cvmfs/pilot.eessi-hpc.org</code> directory.</p> <p>Also, note that there is no shell prompt (<code>Apptainer&gt;</code> or <code>Singularity</code>), since no interactive shell session is started in the container.</p> <p>Alternatively to specify the command as we did above, you can also do the following. <pre><code>CMD=\"ls -l /cvmfs/pilot.eessi-hpc.org\"\n./eessi_container.sh --mode shell &lt;&lt;&lt; ${CMD}\n</code></pre></p> <p>Note</p> <p>We changed the mode from <code>run</code> to <code>shell</code> because we use a different method to let the script run our command, by feeding it in via the <code>stdin</code> input channel using <code>&lt;&lt;&lt;</code>.</p> <p>Because <code>shell</code> is the default value for <code>--mode</code> we can also omit this and simply run <pre><code>CMD=\"ls -l /cvmfs/pilot.eessi-hpc.org\"\n./eessi_container.sh &lt;&lt;&lt; ${CMD}\n</code></pre></p>"},{"location":"getting_access/eessi_container/#running-a-script","title":"Running a script","text":"<p>While running simple command can be sufficient in some cases, you often want to run scripts containing multiple commands.</p> <p>Let's run the script shown below.</p> <p>First, copy-paste the contents for the script shown below, and create a file named <code>eessi_architectures.sh</code> in your current directory. Also make the script executable, by running:</p> <pre><code>chmod +x eessi_architectures.sh\n</code></pre> <p>Here are the contents for the <code>eessi_architectures.sh</code> script:</p> <p><pre><code>#!/usr/bin/env bash\n#\n# This script determines which architectures are included in the\n# latest EESSI pilot version. It makes use of the specific directory\n# structure in the EESSI pilot repository.\n#\n# determine list of available OS types\nBASE=${EESSI_CVMFS_REPO:-/cvmfs/pilot.eessi-hpc.org}/latest/software\ncd ${BASE}\nfor os_type in $(ls -d *)\ndo\n# determine architecture families\nOS_BASE=${BASE}/${os_type}\ncd ${OS_BASE}\nfor arch_family in $(ls -d *)\ndo\n# determine CPU microarchitectures\nOS_ARCH_BASE=${BASE}/${os_type}/${arch_family}\ncd ${OS_ARCH_BASE}\nfor microarch in $(ls -d *)\ndo\ncase ${microarch} in\namd | intel )\nfor sub in $(ls ${microarch})\ndo\necho \"${os_type}/${arch_family}/${microarch}/${sub}\"\ndone\n;;\n* )\necho \"${os_type}/${arch_family}/${microarch}\"\n;;\nesac\ndone\ndone\ndone\n</code></pre> Run the script as follows <pre><code>./eessi_container.sh --mode shell &lt; eessi_architectures.sh\n</code></pre> The output should be similar to <pre><code>Using /tmp/eessi.abc123defg as tmp storage (add '--resume /tmp/eessi.abc123defg' to resume where this session ended).$\nPulling container image from docker://ghcr.io/eessi/build-node:debian11 to /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nLaunching container with command (next line):\nsingularity -q shell --fusemount container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nCernVM-FS: pre-mounted on file descriptor 3\nCernVM-FS: loading Fuse module... done\nlinux/aarch64/generic\nlinux/aarch64/graviton2\nlinux/aarch64/graviton3\nlinux/ppc64le/generic\nlinux/ppc64le/power9le\nlinux/x86_64/amd/zen2\nlinux/x86_64/amd/zen3\nlinux/x86_64/generic\nlinux/x86_64/intel/haswell\nlinux/x86_64/intel/skylake_avx512\n</code></pre> Lines 6 to 15 show the output of the script <code>eessi_architectures.sh</code>.</p> <p>If you want to use the mode <code>run</code>, you have to make the script's location available inside the container.</p> <p>This can be done by mapping the current directory (<code>${PWD}</code>), which contains <code>eessi_architectures.sh</code>, to any not-yet existing directory inside the container using the <code>$SINGULARITY_BIND</code> or <code>$APPTAINER_BIND</code> environment variable.</p> <p>For example: <pre><code>SINGULARITY_BIND=${PWD}:/scripts ./eessi_container.sh --mode run /scripts/eessi_architectures.sh\n</code></pre></p>"},{"location":"getting_access/eessi_container/#running-scripts-or-commands-with-parameters-starting-with-or-","title":"Running scripts or commands with parameters starting with <code>-</code> or <code>--</code>","text":"<p>Let's assume we would like to get more information about the entries of <code>/cvmfs/pilot.eessi-hpc.org</code>. If we would just run <pre><code>./eessi_container.sh --mode run ls -lH /cvmfs/pilot.eessi-hpc.org\n</code></pre> we would get an error message such as <pre><code>ERROR: Unknown option: -lH\n</code></pre> We can resolve this in two ways:</p> <ol> <li>Using the <code>stdin</code> channel as described above, for example, by simply running   <pre><code>CMD=\"ls -lH /cvmfs/pilot.eessi-hpc.org\"\n./eessi_container.sh &lt;&lt;&lt; ${CMD}\n</code></pre>   which should result in the output similar to   <pre><code>Using /tmp/eessi.abc123defg as tmp directory (to resume session add '--resume /tmp/eessi.abc123defg').\nPulling container image from docker://ghcr.io/eessi/build-node:debian11 to /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nLaunching container with command (next line):\nsingularity -q shell --fusemount container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nCernVM-FS: pre-mounted on file descriptor 3\nCernVM-FS: loading Fuse module... done\nfuse: failed to clone device fd: Inappropriate ioctl for device\nfuse: trying to continue without -o clone_fd.\ntotal 10\nlrwxrwxrwx 1 user user   10 Jun 30  2021 host_injections -&gt; /opt/eessi\nlrwxrwxrwx 1 user user   16 May  4  2022 latest -&gt; versions/2021.12\ndrwxr-xr-x 3 user user 4096 Dec 10  2021 versions\n</code></pre></li> <li>Using the flag terminator <code>--</code> which tells <code>eessi_container.sh</code> to stop parsing command line arguments. For example,   <pre><code>./eessi_container.sh --mode run -- ls -lH /cvmfs/pilot.eessi-hpc.org\n</code></pre>   which should result in the output similar to   <pre><code>Using /tmp/eessi.abc123defg as tmp directory (to resume session add '--resume /tmp/eessi.abc123defg').\nPulling container image from docker://ghcr.io/eessi/build-node:debian11 to /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nLaunching container with command (next line):\nsingularity -q run --fusemount container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif ls -lH /cvmfs/pilot.eessi-hpc.org\nCernVM-FS: pre-mounted on file descriptor 3\nCernVM-FS: loading Fuse module... done\nfuse: failed to clone device fd: Inappropriate ioctl for device\nfuse: trying to continue without -o clone_fd.\ntotal 10\nlrwxrwxrwx 1 user user   10 Jun 30  2021 host_injections -&gt; /opt/eessi\nlrwxrwxrwx 1 user user   16 May  4  2022 latest -&gt; versions/2021.12\ndrwxr-xr-x 3 user user 4096 Dec 10  2021 versions\n</code></pre></li> </ol>"},{"location":"getting_access/eessi_container/#running-eessi-demos","title":"Running EESSI demos","text":"<p>For examples of scripts that use the software provided by EESSI, see Running EESSI demos.</p>"},{"location":"getting_access/eessi_container/#launching-containers-more-quickly","title":"Launching containers more quickly","text":"<p>Subsequent runs of <code>eessi_container.sh</code> may reuse temporary data of a previous session, which includes the pulled image of the container. However, that is not always what we want, i.e., reusing a previous session (and thereby launching the container more quickly).</p> <p>The <code>eessi_container.sh</code> script may (re)-use a cache directory provided via <code>$SINGULARITY_CACHEDIR</code> (or <code>$APPTAINER_CACHEDIR</code> when using Apptainer). Hence, the container image does not have to be downloaded again even when starting a new session. The example below illustrates this. <pre><code>export SINGULARITY_CACHEDIR=${PWD}/container_cache_dir\ntime ./eessi_container.sh &lt;&lt;&lt; \"ls /cvmfs/pilot.eessi-hpc.org\"\n</code></pre> which should produce output similar to <pre><code>Using /tmp/eessi.abc123defg as tmp directory (to resume session add '--resume /tmp/eessi.abc123defg').\nPulling container image from docker://ghcr.io/eessi/build-node:debian11 to /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nLaunching container with command (next line):\nsingularity -q shell --fusemount container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nCernVM-FS: pre-mounted on file descriptor 3\nCernVM-FS: loading Fuse module... done\nfuse: failed to clone device fd: Inappropriate ioctl for device\nfuse: trying to continue without -o clone_fd.\nhost_injections  latest  versions\nreal    m40.445s\nuser    3m2.621s\nsys     0m7.402s\n</code></pre> The next run using the same cache directory, e.g., by simply executing <pre><code>time ./eessi_container.sh &lt;&lt;&lt; \"ls /cvmfs/pilot.eessi-hpc.org\"\n</code></pre> is much faster <pre><code>Using /tmp/eessi.abc123defg as tmp directory (to resume session add '--resume /tmp/eessi.abc123defg').\nPulling container image from docker://ghcr.io/eessi/build-node:debian11 to /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nLaunching container with command (next line):\nsingularity -q shell --fusemount container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org /tmp/eessi.abc123defg/ghcr.io_eessi_build_node_debian11.sif\nCernVM-FS: pre-mounted on file descriptor 3\nCernVM-FS: loading Fuse module... done\nfuse: failed to clone device fd: Inappropriate ioctl for device\nfuse: trying to continue without -o clone_fd.\nhost_injections  latest  versions\nreal    0m2.781s\nuser    0m0.172s\nsys     0m0.436s\n</code></pre></p> <p>Note</p> <p>Each run of <code>eessi_container.sh</code> (without specifying <code>--resume</code>) creates a new temporary directory. The temporary directory stores, among other data, the image file of the container. Thus we can ensure that the container is available locally for a subsequent run.</p> <p>However, this may quickly consume scarce resources, for example, a small partition where <code>/tmp</code> is located (default for temporary storage, see <code>--help</code> for specifying a different location).</p> <p>See next section for making sure to clean up no longer needed temporary data.</p>"},{"location":"getting_access/eessi_container/#reducing-disk-usage","title":"Reducing disk usage","text":"<p>By default <code>eessi_container.sh</code> creates a temporary directory under <code>/tmp</code>. The directories are named <code>eessi.RANDOM</code> where <code>RANDOM</code> is a 10-character string. The script does not automatically remove these directories. To determine their total disk usage, simply run <pre><code>du -sch /tmp/eessi.*\n</code></pre> which could result in output similar to <pre><code>333M    /tmp/eessi.session123\n333M    /tmp/eessi.session456\n333M    /tmp/eessi.session789\n997M    total\n</code></pre> Clean up disk usage by simply removing directories you do not need any longer.</p>"},{"location":"getting_access/is_eessi_accessible/","title":"Is EESSI accessible?","text":"<p>EESSI can be accessed via a native (CernVM-FS) installation, or via a container that includes CernVM-FS.</p> <p>Before you look into these options, check if EESSI is already accessible on your system.</p> <p>Run the following command: <pre><code>ls /cvmfs/pilot.eessi-hpc.org\n</code></pre></p> <p>Note</p> <p>This <code>ls</code> command may take a couple of seconds to finish, since CernVM-FS may need to download or update the metadata for that directory.</p> <p>If you see output like shown below, you already have access to EESSI on your system.  <pre><code>host_injections  latest  versions\n</code></pre></p> <p>For starting to use EESSI, continue reading about Setting up environment.</p> <p>If you see an error message as shown below, EESSI is not yet accessible on your system. <pre><code>ls: /cvmfs/pilot.eessi-hpc.org: No such file or directory\n</code></pre> No worries, you don't need to be a  to get access to EESSI.</p> <p>Continue reading about the Native installation of EESSI, or access via the EESSI container.</p>"},{"location":"getting_access/native_installation/","title":"Native installation","text":"<p>Setting up native access to EESSI, that is a system-wide deployment that does not require workarounds like using a container, requires the installation and configuration of CernVM-FS.</p> <p>This requires admin privileges, since you need to install CernVM-FS as an OS package.</p> <p>The following actions must be taken for a (basic) native installation of EESSI:</p> <ul> <li>Installing CernVM-FS itself, ideally using the OS packages provided by the CernVM-FS project   (although installing from source is also possible);</li> <li>Installing the EESSI configuration for CernVM-FS, which can be done by installing the <code>cvmfs-config-eessi</code>   package that we provide for the most popular Linux distributions   (more information available here);</li> <li>Creating a small client configuration file for CernVM-FS (<code>/etc/cvmfs/default.local</code>);   see also the CernVM-FS documentation.</li> </ul> <p>The good news is that all of this only requires a handful commands  :</p> RHEL-based Linux distributionsDebian-based Linux distributions <pre><code># Installation commands for RHEL-based distros like CentOS, Rocky Linux, Almalinux, Fedora, ...\n# install CernVM-FS\nsudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm\nsudo yum install -y cvmfs\n\n# install EESSI configuration for CernVM-FS\nsudo yum install -y https://github.com/EESSI/filesystem-layer/releases/download/latest/cvmfs-config-eessi-latest.noarch.rpm\n\n# create client configuration file for CernVM-FS (no squid proxy, 10GB local CernVM-FS client cache)\nsudo bash -c \"echo 'CVMFS_CLIENT_PROFILE=\"single\"' &gt; /etc/cvmfs/default.local\"\nsudo bash -c \"echo 'CVMFS_QUOTA_LIMIT=10000' &gt;&gt; /etc/cvmfs/default.local\"\n# make sure that EESSI CernVM-FS repository is accessible\nsudo cvmfs_config setup\n</code></pre> <pre><code># Installation commands for Debian-based distros like Ubuntu, ...\n# install CernVM-FS\nsudo apt-get install lsb-release\nwget https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest_all.deb\nsudo dpkg -i cvmfs-release-latest_all.deb\nrm -f cvmfs-release-latest_all.deb\nsudo apt-get update\nsudo apt-get install -y cvmfs\n\n# install EESSI configuration for CernVM-FS\nwget https://github.com/EESSI/filesystem-layer/releases/download/latest/cvmfs-config-eessi_latest_all.deb\nsudo dpkg -i cvmfs-config-eessi_latest_all.deb\n\n# create client configuration file for CernVM-FS (no squid proxy, 10GB local CernVM-FS client cache)\nsudo bash -c \"echo 'CVMFS_CLIENT_PROFILE=\"single\"' &gt; /etc/cvmfs/default.local\"\nsudo bash -c \"echo 'CVMFS_QUOTA_LIMIT=10000' &gt;&gt; /etc/cvmfs/default.local\"\n# make sure that EESSI CernVM-FS repository is accessible\nsudo cvmfs_config setup\n</code></pre> <p>Note</p> <p> The commands above only cover the basic installation of EESSI.</p> <p>This is good enough for an individual client, or for testing purposes, but for a production-quality setup you should also set up a Squid proxy cache.</p> <p>For large-scale systems, like an HPC cluster, you should also consider setting up your own CernVM-FS Stratum-1 mirror server.</p> <p>For more details on this, please refer to the Stratum 1 and proxies section of the CernVM-FS tutorial.</p>"},{"location":"meetings/2022-09-amsterdam/","title":"EESSI Community Meeting (Sept'22, Amsterdam)","text":""},{"location":"meetings/2022-09-amsterdam/#practical-info","title":"Practical info","text":"<ul> <li>dates: Wed-Fri 14-16 Sept'22</li> <li>in conjunction with CernVM workshop @ Nikhef (Mon-Tue 12-13 Sept'22)</li> <li>venue: \"Polderzaal\" at Cafe-Restaurant Polder (Google   Maps), sponsored by SURF</li> <li>registration (closed since Fri 9 Sept'22)</li> <li>Slack channel: <code>community-meeting-2022</code> in EESSI Slack</li> <li>YouTube playlist with recorded talks</li> </ul>"},{"location":"meetings/2022-09-amsterdam/#agenda","title":"Agenda","text":"<p>(subject to changes)</p> <p>We envision a mix of presentations, experience reports, demos, and hands-on sessions and/or hackathons related to the EESSI project.</p> <p>If you would like to give a talk or host a session, please let us know via the EESSI Slack!</p>"},{"location":"meetings/2022-09-amsterdam/#wed-14-sept-2022","title":"Wed 14 Sept 2022","text":"<ul> <li>[10:00-13:00] Welcome session<ul> <li>[10:00-10:30] Walk-in, coffee</li> <li>[10:30-12:00] Round table discussion (not live-streamed!)</li> </ul> </li> <li>[12:00-13:00] Lunch</li> <li>[13:00-15:00] Presentations on EESSI<ul> <li>[13:00-13:30] Introduction to EESSI (Caspar) [slides - recording]</li> <li>[13:30-14:00] Hands-on: how to use EESSI (Kenneth) [slides - recording]</li> <li>[14:00-14:30] EESSI use cases (Kenneth) [(slides - recording]</li> <li>[14:30-15:00] EESSI for sysadmins (Thomas) [slides - recording]</li> </ul> </li> <li>[15:00-15:30] Coffee break</li> <li>[15:30-17:00] Presentations on EESSI (continued)<ul> <li>[15:30-16:00] Hands-on: installing EESSI (Thomas/Kenneth)</li> <li>[16:00-16:45] ComputeCanada site talk (Bart Oldeman, remote) [slides - recording]</li> <li>[16:45-17:15] Magic Castle (Felix-Antoine Fortin, remote) [slides - recording]</li> </ul> </li> <li>[19:00-...] Group dinner @ Saravanaa Bhavan (sponsored by Dell Technologies)<ul> <li>address: Stadhouderskade 123-124, Amsterdam</li> </ul> </li> </ul>"},{"location":"meetings/2022-09-amsterdam/#thu-15-sept-2022","title":"Thu 15 Sept 2022","text":"<ul> <li>[09:30-12:00] More focused presentations on aspects of EESSI<ul> <li>[09:30-10:00] EESSI behind the scenes: compat layer (Bob) [slides - recording]</li> <li>[10:00-10:30] EESSI behind the scenes: software layer (Kenneth) [slides - recording]</li> <li>[10:30-11:00] Coffee break</li> <li>[11:00-11:30] EESSI behind the scenes: infrastructure (Terje) [slides - recording]</li> <li>[11:30-12:00] Status on RISC-V support (Kenneth) [slides - recording]</li> </ul> </li> <li>[12:00-13:00] Lunch</li> <li>[13:00-14:00] Discussions/hands-on sessions/hackathon</li> <li>[14:00-14:30] Status on GPU support (Alan) [slides - recording]</li> <li>[14:30-15:00] Status on build-and-deploy bot (Thomas) [slides - recording]</li> <li>[15:00-15:30] Coffee break</li> <li>[15:30-17:00] Discussions/hands-on sessions/hackathon (continued)<ul> <li>Hands-on with GPUs (Alan)</li> <li>Hands-on with bot (Thomas/Kenneth)</li> </ul> </li> <li>[19:00-...] Group dinner @ Italia Oggi (sponsored by HPC-UGent)<ul> <li>address: Binnen Bantammerstraat 11, Amsterdam</li> </ul> </li> </ul>"},{"location":"meetings/2022-09-amsterdam/#fri-16-sept-2022","title":"Fri 16 Sept 2022","text":"<ul> <li>[09:30-12:00] Presentations on future work<ul> <li>[09:30-10:00] Testing in software layer (Caspar) [slides - recording]</li> <li>[10:00-10:30] MultiXscale project (Alan) [slides - recording]</li> <li>[10:30-11:00] Coffee break</li> <li>[11:00-11:30] Short-term future work (Kenneth) [slides - recording]</li> </ul> </li> <li>[11:30-12:00] Discussion: future management structure of EESSI (Alan) [slides - recording]</li> <li>[12:00-13:00] Lunch</li> <li>[13:00-14:00] Site reports [recording]<ul> <li>NESSI (Thomas) [slides]</li> <li>NLPL (Stephan) [slides]</li> <li>HPCNow! (Danilo) [slides]</li> <li>Azure (Hugo) [slides]</li> </ul> </li> <li>[14:00-14:30] Discussion: what would make or break EESSI for your site? (notes - recording)</li> <li>[14:30-15:45] Discussions/hands-on sessions/hackathon<ul> <li>Hands-on with GPU support (Alan)</li> <li>Hands-on with bot (Thomas/Kenneth)</li> <li>Hands-on with software testing (Caspar)</li> </ul> </li> <li>We need to leave the room by 16:00!</li> </ul>"},{"location":"software_layer/adding_software/","title":"Adding software","text":"<p>To add software to EESSI, you should go through the semi-automatic software installation procedure by:</p> <ul> <li>1) Making a pull request to the software-layer repository      to (add or) update an easystack file  that is used by      EasyBuild to install software;</li> <li>2) Instructing the bot  to build the software on all supported CPU microarchitectures;</li> <li>3) Instructing the bot  to deploy the built software for ingestion into the EESSI repository;</li> <li>4) Merging the pull request once CI indicates that the software has been ingested. </li> </ul>"},{"location":"software_layer/adding_software/#preparation","title":"Preparation","text":"<p>Before you can make a pull request to the software-layer, you should fork the repository in your GitHub account.</p> <p>For the remainder of these instructions, we assume that your GitHub account is <code>@koala</code> .</p> <p>Note</p> <p>Don't forget to replace <code>koala</code>  with the name of your GitHub account in the commands below!</p> <p>1) Clone the EESSI/software-layer repository:</p> <pre><code>mkdir EESSI\ncd EESSI\ngit clone https://github.com/EESSI/software-layer\ncd software-layer\n</code></pre> <p>2) Add your fork  as a remote</p> <pre><code>git remote add koala git@github.com:koala/software-layer.git\n</code></pre> <p>3) Check out the branch that corresponds to the version of EESSI repository you want to add software to,    for example <code>2023.06</code>:</p> <pre><code>git checkout 2023.06\n</code></pre> <p>Note</p> <p>The commands above only need to be run once, to prepare your setup for making pull requests.</p>"},{"location":"software_layer/adding_software/#software_layer_pull_request","title":"Creating a pull request","text":"<p>1) Make sure that your <code>2023.06</code> branch in the checkout of the   <code>EESSI/software-layer</code> repository is up-to-date</p> <pre><code>cd EESSI/software-layer\ngit checkout 2023.06\ngit pull origin 2023.06\n</code></pre> <p>2) Create a new branch (use a sensible name, not <code>example_branch</code> as below), and check it out</p> <pre><code>git checkout -b example_branch\n</code></pre> <p>3) Determine the correct easystack file to change, and add one or more lines to it that specify which    easyconfigs should be installed</p> <pre><code>echo '  - example-1.2.3-GCC-10.3.0.eb' &gt;&gt; eessi-2023.06-eb-4.7.2-2021a.yml\n</code></pre> <p>4) Stage and commit the changes into your your branch with a sensible message</p> <pre><code>git add eessi-2023.06-eb-4.7.2-2021a.yml\ngit commit -m \"adding example 1.2.3 with GCC/10.3.0 to EESSI pilot 2023.06\"\n</code></pre> <p>5) Push your branch to your fork  of the software-layer repository</p> <pre><code>git push koala example_branch\n</code></pre> <p>6) Go to the GitHub web interface to open your pull request,    or use the helpful link that should show up in the output of the <code>git push</code> command.</p> <p>Make sure you target the correct branch: the one that corresponds to the version of EESSI you want to add    software to (like <code>2023.06</code>).</p> <p>If all goes well, one or more bots  should almost instantly create a comment in your pull request    with an overview of how it is configured - you will need this information when providing build instructions.</p>"},{"location":"software_layer/adding_software/#bot_build","title":"Instructing the bot to build","text":"<p>Once the pull request is open, you can instruct the bot  to build the software by posting a comment.</p> <p>For more information, see the building section in the bot documentation.</p> <p>Warning</p> <p>Permission to trigger building of software must be granted to your GitHub account first!</p> <p>See bot permissions for more information.</p>"},{"location":"software_layer/adding_software/#guidelines","title":"Guidelines","text":"<ul> <li> <p>It may be wise to let the bot perform a test build first, rather than letting it build for a wide range   of CPU targets.</p> </li> <li> <p>If one of the builds failed, you can let the bot retry that specific build.</p> </li> <li> <p>Make sure that the software has been built correctly for all CPU targets before you deploy!</p> </li> </ul>"},{"location":"software_layer/adding_software/#checking-the-builds","title":"Checking the builds","text":"<p>If all goes well, you should see <code>SUCCESS</code>  for each build, along with button  to get more information about the checks that were performed, and metadata information on the resulting artefact .</p> <p>Note</p> <p>Make sure the result is what you expect it to be for all builds before you deploy!</p>"},{"location":"software_layer/adding_software/#failing-builds","title":"Failing builds","text":"<p>Warning</p> <p>The bot will currently not give you any information on how or why a build is failing.</p> <p>Ask for help in the <code>#software-layer</code> channel of the EESSI Slack if needed!</p>"},{"location":"software_layer/adding_software/#instructing-the-bot-to-deploy","title":"Instructing the bot to deploy","text":"<p>To make the bot  deploy the successfully built software, you should issue the corresponding instruction to the bot.</p> <p>For more information, see the deploying section in the bot documentation.</p> <p>Warning</p> <p>Permission to trigger deployment of software installations must be granted to your GitHub account first!</p> <p>See bot permissions for more information.</p>"},{"location":"software_layer/adding_software/#merging-the-pull-request","title":"Merging the pull request","text":"<p>You should be able to verify in the pull request that the ingestion has been done, since the CI should fail  initially to indicate that some software installations listed in your modified easystack are missing.</p> <p>Once the ingestion has been done, simply re-triggering the CI workflow should be sufficient to make it pass , and then the pull request can be merged.</p> <p>Note</p> <p>This assumes that the easystack file being modified is considered by the CI workflow file (<code>.github/workflows/test_eessi.yml</code>) that checks for missing installations, in the correct branch (for example <code>2023.06</code>) of the software-layer.</p> <p>If that's not the case yet, update this workflow in your pull request as well to add the missing easystack file!</p> <p>Warning</p> <p>You need permissions to re-trigger CI workflows and merge pull requests in the software-layer repository.</p> <p>Ask for help in the <code>#software-layer</code> channel of the EESSI Slack if needed!</p>"},{"location":"software_layer/adding_software/#getting-help","title":"Getting help","text":"<p>If you have any questions, or if you need help with something, don't hesitate to contact us via the <code>#software-layer</code> channel of the EESSI Slack.</p>"},{"location":"software_layer/build_nodes/","title":"Build nodes","text":"<p>Any system can be used as a build node to create additional software installations that should be added to the EESSI CernVM-FS repository.</p>"},{"location":"software_layer/build_nodes/#requirements","title":"Requirements","text":"<p>OS and software:</p> <ul> <li>GNU/Linux (any distribution) as operating system;</li> <li>a recent version of Singularity (&gt;= 3.6 is recommended);<ul> <li>check with <code>singularity --version</code></li> </ul> </li> <li><code>screen</code> or <code>tmux</code> is highly recommended;</li> </ul> <p>Admin privileges are not required, as long as Singularity is installed.</p> <p>Resources:</p> <ul> <li>8 or more cores is recommended (though not strictly required);</li> <li>at least 50GB of free space on a local filesystem (like <code>/tmp</code>);</li> <li>at least 16GB of memory (2GB/core or higher recommended);</li> </ul> <p>Instructions to install Singularity and screen (click to show commands):</p> CentOS 8 (<code>x86_64</code> or <code>aarch64</code> or <code>ppc64le</code>) <pre><code>sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm\nsudo dnf update -y\nsudo dnf install -y screen singularity\n</code></pre>"},{"location":"software_layer/build_nodes/#setting-up-the-container","title":"Setting up the container","text":"<p>Warning</p> <p>It is highly recommended to start a <code>screen</code> or <code>tmux</code> session first!</p> <p>A container image is provided that includes everything that is required to set up a writable overlay on top of the EESSI CernVM-FS repository.</p> <p>First, pick a location on a local filesystem for the temporary directory:</p> <p>Requirements:</p> <ul> <li>Do not use a shared filesystem like NFS, Lustre or GPFS.</li> <li>There should be at least 50GB of free disk space in this local filesystem (more is better).</li> <li>There should be no automatic cleanup of old files via a cron job on this local filesystem.</li> <li>Try to make sure the directory is unique (not used by anything else).</li> </ul> <p>NB. If you are going to install on a separate drive (due to lack of space on /), then you need to set some variables to  point to that location. You will also need to bind mount it in the <code>singularity</code> command. Let's say that you drive is  mounted in /srt. Then you change the relevant commands below to this: <pre><code>export EESSI_TMPDIR=/srt/$USER/EESSI\nmkdir -p $EESSI_TMPDIR\nmkdir /srt/tmp\nexport SINGULARITY_BIND=\"$EESSI_TMPDIR/var-run-cvmfs:/var/run/cvmfs,$EESSI_TMPDIR/var-lib-cvmfs:/var/lib/cvmfs,/srt/tmp:/tmp\"\nsingularity shell -B /srt --fusemount \"$EESSI_PILOT_READONLY\" --fusemount \"$EESSI_PILOT_WRITABLE_OVERLAY\" docker://ghcr.io/eessi/build-node:debian10\n</code></pre></p> <p>We will assume that <code>/tmp/$USER/EESSI</code> meets these requirements:</p> <pre><code>export EESSI_TMPDIR=/tmp/$USER/EESSI\nmkdir -p $EESSI_TMPDIR\n</code></pre> <p>Create some subdirectories in this temporary directory:</p> <pre><code>mkdir -p $EESSI_TMPDIR/{home,overlay-upper,overlay-work}\nmkdir -p $EESSI_TMPDIR/{var-lib-cvmfs,var-run-cvmfs}\n</code></pre> <p>Configure Singularity cache directory, bind mounts, and (fake) home directory:</p> <pre><code>export SINGULARITY_CACHEDIR=$EESSI_TMPDIR/singularity_cache\nexport SINGULARITY_BIND=\"$EESSI_TMPDIR/var-run-cvmfs:/var/run/cvmfs,$EESSI_TMPDIR/var-lib-cvmfs:/var/lib/cvmfs\"\nexport SINGULARITY_HOME=\"$EESSI_TMPDIR/home:/home/$USER\"\n</code></pre> <p>Define values to pass to <code>--fusemount` in</code>singularity`` command:</p> <pre><code>export EESSI_PILOT_READONLY=\"container:cvmfs2 pilot.eessi-hpc.org /cvmfs_ro/pilot.eessi-hpc.org\"\nexport EESSI_PILOT_WRITABLE_OVERLAY=\"container:fuse-overlayfs -o lowerdir=/cvmfs_ro/pilot.eessi-hpc.org -o upperdir=$EESSI_TMPDIR/overlay-upper -o workdir=$EESSI_TMPDIR/overlay-work /cvmfs/pilot.eessi-hpc.org\"\n</code></pre> <p>Start the container (which includes Debian 10, CernVM-FS and fuse-overlayfs):</p> <pre><code>singularity shell --fusemount \"$EESSI_PILOT_READONLY\" --fusemount \"$EESSI_PILOT_WRITABLE_OVERLAY\" docker://ghcr.io/eessi/build-node:debian10\n</code></pre> <p>Once the container image has been downloaded and converted to a Singularity image (SIF format), you should get a prompt like this:</p> <pre><code>...\nCernVM-FS: loading Fuse module... done\n\nSingularity&gt;\n</code></pre> <p>and the EESSI CernVM-FS repository should be mounted:</p> <pre><code>Singularity&gt; ls /cvmfs/pilot.eessi-hpc.org\n2020.12  2021.03  latest\n</code></pre>"},{"location":"software_layer/build_nodes/#setting-up-the-environment","title":"Setting up the environment","text":"<p>Set up the environment by starting a Gentoo Prefix session using the <code>startprefix</code> command.</p> <p>Make sure you use the correct version of the EESSI pilot repository!</p> <pre><code>export EESSI_PILOT_VERSION='2021.03'\n/cvmfs/pilot.eessi-hpc.org/${EESSI_PILOT_VERSION}/compat/linux/$(uname -m)/startprefix\n</code></pre>"},{"location":"software_layer/build_nodes/#installing-software","title":"Installing software","text":"<p>Clone the software-layer repository:</p> <pre><code>git clone https://github.com/EESSI/software-layer.git\n</code></pre> <p>Run the software installation script in <code>software-layer</code>:</p> <pre><code>cd software-layer\n./EESSI-pilot-install-software.sh\n</code></pre> <p>This script will figure out the CPU microarchitecture of the host automatically (like <code>x86_64/intel/haswell</code>).</p> <p>To build generic software installations (like <code>x86_64/generic</code>), use the <code>--generic</code> option:</p> <pre><code>./EESSI-pilot-install-software.sh --generic\n</code></pre> <p>Once all missing software has been installed, you should see a message like this:</p> <pre><code>No missing modules!\n</code></pre>"},{"location":"software_layer/build_nodes/#creating-tarball-to-ingest","title":"Creating tarball to ingest","text":"<p>Before tearing down the build node, you should create tarball to ingest into the EESSI CernVM-FS repository.</p> <p>To create a tarball of all installations, assuming your build host is <code>x86_64/intel/haswell</code>:</p> <pre><code>export EESSI_PILOT_VERSION='2021.03'\ncd /cvmfs/pilot.eessi-hpc.org/${EESSI_PILOT_VERSION}/software/linux\neessi_tar_gz=\"$HOME/eessi-${EESSI_PILOT_VERSION}-haswell.tar.gz\"\ntar cvfz ${eessi_tar_gz} x86_64/intel/haswell\n</code></pre> <p>To create a tarball for specific installations, make sure you pick up both the software installation directories and the corresponding module files:</p> <pre><code>eessi_tar_gz=\"$HOME/eessi-${EESSI_PILOT_VERSION}-haswell-OpenFOAM.tar.gz\"\ntar cvfz ${eessi_tar_gz} x86_64/intel/haswell/software/OpenFOAM modules/all//OpenFOAM\n</code></pre> <p>This tarball should be uploaded to the Stratum 0 server for ingestion. If needed, you can ask for help in the EESSI <code>#software-layer</code> Slack channel</p>"},{"location":"software_layer/cpu_targets/","title":"CPU targets","text":"<p>In the 2023.06 version of the EESSI pilot repository, the following CPU microarchitectures are supported.</p> <ul> <li><code>aarch64/generic</code>: fallback for Arm 64-bit CPUs (like Raspberri Pi, etc.)</li> <li><code>aarch64/neoverse_n1</code>: AWS Graviton 2, Ampere Altra, ...</li> <li><code>aarch64/neoverse_v1</code>: AWS Graviton 3</li> <li><code>x86_64/generic</code>: fallback for older Intel + AMD CPUs (like Intel Sandy Bridge, ...)</li> <li><code>x86_64/amd/zen2</code>: AMD Rome</li> <li><code>x86_64/amd/zen3</code>: AMD Milan, AMD Milan X</li> <li><code>x86_64/intel/haswell</code>: Intel Haswell, Broadwell</li> <li><code>x86_64/intel/skylake_avx512</code>: Intel Skylake, Cascade Lake, Ice Lake, ...</li> </ul> <p>The names of these CPU targets correspond to the names used by archspec.</p>"},{"location":"using_eessi/basic_commands/","title":"Basic commands","text":""},{"location":"using_eessi/basic_commands/#basic-commands-to-access-software-provided-via-eessi","title":"Basic commands to access software provided via EESSI","text":"<p>EESSI provides software through environment module files and Lmod.</p> <p>To see which modules (and extensions) are available, run:</p> <pre><code>module avail\n</code></pre> <p>Below is a short excerpt of the output produced by <code>module avail</code>, showing 10 modules only. <pre><code>   PyYAML/5.3-GCCcore-9.3.0\n   Qt5/5.14.1-GCCcore-9.3.0\n   Qt5/5.15.2-GCCcore-10.3.0                               (D)\n   QuantumESPRESSO/6.6-foss-2020a\n   R-bundle-Bioconductor/3.11-foss-2020a-R-4.0.0\n   R/4.0.0-foss-2020a\n   R/4.1.0-foss-2021a                                      (D)\n   re2c/1.3-GCCcore-9.3.0\n   re2c/2.1.1-GCCcore-10.3.0                               (D)\n   RStudio-Server/1.3.1093-foss-2020a-Java-11-R-4.0.0\n</code></pre></p> <p>Load modules with <code>module load package/version</code>, e.g., <code>module load R/4.1.0-foss-2021a</code>, and try out the software. See below for a short session</p> <pre><code>[EESSI pilot 2021.12] $ module load R/4.1.0-foss-2021a\n[EESSI pilot 2021.12] $ which R\n/cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/skylake_avx512/software/R/4.1.0-foss-2021a/bin/R\n[EESSI pilot 2021.12] $ R --version\nR version 4.1.0 (2021-05-18) -- \"Camp Pontanezen\"\nCopyright (C) 2021 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under the terms of the\nGNU General Public License versions 2 or 3.\nFor more information about these matters see\nhttps://www.gnu.org/licenses/.\n</code></pre>"},{"location":"using_eessi/eessi_demos/","title":"Running EESSI demos","text":"<p>To really experience how using EESSI can significantly facilitate the work of researchers, we recommend running one or more of the EESSI demos.</p> <p>First, clone the <code>eessi-demo</code> Git repository, and move into the resulting directory:</p> <pre><code>git clone https://github.com/EESSI/eessi-demo.git\ncd eessi-demo\n</code></pre> <p>The contents of the directory should be something like this:</p> <pre><code>$ ls -l\ndrwxr-xr-x  5 example  users    160 Nov 23  2020 Bioconductor\ndrwxr-xr-x  3 example  users     96 Jan 26 20:17 CitC\ndrwxr-xr-x  5 example  users    160 Jan 26 20:17 GROMACS\n-rw-r--r--  1 example  users  18092 Jan 26 20:17 LICENSE\ndrwxr-xr-x  3 example  users     96 Jan 26 20:17 Magic_Castle\ndrwxr-xr-x  4 example  users    128 Nov 24  2020 OpenFOAM\n-rw-r--r--  1 example  users    546 Jan 26 20:17 README.md\ndrwxr-xr-x  5 example  users    160 Nov 23  2020 TensorFlow\ndrwxr-xr-x  6 example  users    192 Jan 26 20:17 scripts\n</code></pre> <p>The directories we care about are those that correspond to particular scientific software, like <code>Bioconductor</code>, <code>GROMACS</code>, <code>OpenFOAM</code>, <code>TensorFlow</code>, ...</p> <p>Each of these contains a <code>run.sh</code> script that can be used to start a small example run with that software. Every example takes a couple of minutes to run, even with limited resources only.</p>"},{"location":"using_eessi/eessi_demos/#example-running-gromacs","title":"Example: running GROMACS","text":"<p>Let's try running the GROMACS example.</p> <p>First, we need to make sure that our environment is set up to use EESSI:</p> <pre><code>source /cvmfs/pilot.eessi-hpc.org/latest/init/bash\n</code></pre> <p>Change to the <code>GROMACS</code> subdirectory of the <code>eessi-demo</code> Git repository, and execute the <code>run.sh</code> script:</p> <pre><code>[EESSI pilot 2021.12] $ cd GROMACS\n[EESSI pilot 2021.12] $ ./run.sh\n</code></pre> <p>Shortly after starting the script you should see output as shown below, which indicates that GROMACS has started running:</p> <pre><code>GROMACS:      gmx mdrun, version 2020.1-EasyBuild-4.5.0\nExecutable:   /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/software/GROMACS/2020.1-foss-2020a-Python-3.8.2/bin/gmx\n...\nstarting mdrun 'Protein'\n1000 steps,      2.5 ps.\n</code></pre>"},{"location":"using_eessi/setting_up_environment/","title":"Setting up your environment","text":"<p>To set up the EESSI environment, simply run the command:</p> <pre><code>source /cvmfs/pilot.eessi-hpc.org/latest/init/bash\n</code></pre> <p>Warning</p> <p>The EESSI pilot software stack is NOT READY FOR PRODUCTION!</p> <p>Do not use it for production work, and be careful when testing it on production systems!</p> <p>This may take a while as data is downloaded from a Stratum 1 server which is part of the CernVM-FS infrastructure to distribute files. You should see the following output:</p> <pre><code>Found EESSI pilot repo @ /cvmfs/pilot.eessi-hpc.org/versions/2021.12!\narchspec says x86_64/intel/skylake_avx512 # (1)!\nUsing x86_64/intel/skylake_avx512 as software subdirectory.\nUsing /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/skylake_avx512/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/skylake_avx512/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/skylake_avx512/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI pilot software stack, have fun!\n[EESSI pilot 2021.12] $ # (2)!\n</code></pre> <ol> <li>What is reported here depends on the CPU architecture of the machine you are     running the <code>source</code> command.</li> <li>This is the prompt indicating that you have access to the EESSI software     stack.</li> </ol> <p>The last line is the shell prompt.</p> <p> Your environment is now set up, you are ready to start running software provided by EESSI!</p>"}]}