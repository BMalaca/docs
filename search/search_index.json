{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-eessi-project-documentation","title":"Welcome to the EESSI project documentation!","text":"<p>Quote</p> <p>What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance?</p> <p>The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community.</p> <p>The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure.</p> <p>More details about the project are available in the different subsections:</p> <ul> <li>Project overview</li> <li>Filesystem layer</li> <li>Compatibility layer</li> <li>Software layer</li> <li>Pilot repository</li> <li>Software testing</li> <li>Meetings</li> <li>Project partners</li> <li>Contact info</li> </ul> <p>The EESSI project was presented at the 6th EasyBuild User Meeting in January 2021, check the recording:</p>"},{"location":"compatibility_layer/","title":"Compatibility layer","text":"<p>The middle layer of the EESSI project is the compatibility layer, which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL).</p> <p>For this we rely on Gentoo Prefix, by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage.</p> <p>The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.</p>"},{"location":"contact/","title":"Contact info","text":"<p>For more information:</p> <ul> <li>visit our website https://www.eessi-hpc.org</li> <li>consult our documentation at https://eessi.github.io</li> <li>reach out to one of the project partners</li> <li>check out our GitHub repositories at https://github.com/EESSI</li> <li>follow us on Twitter: https://twitter.com/eessi_hpc</li> </ul> <p>A Slack channel is available for the EESSI community (an invitation is required to join).</p> <p></p>"},{"location":"filesystem_layer/","title":"Filesystem layer","text":"<p>The bottom layer of the EESSI project is the filesystem layer, which is responsible for distributing the software stack.</p> <p>For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way.</p> <p>CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers.</p> <p></p> <p>The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module.</p> <p>For a (basic) introduction to CernVM-FS, see this presentation.</p> <p>Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .</p>"},{"location":"meetings/","title":"Meetings","text":""},{"location":"meetings/#monthly-meetings-online","title":"Monthly meetings (online)","text":"<p>Online EESSI update meeting, every 1st Thursday of the month at 14:00 CE(S)T.</p> <p>More info via https://github.com/EESSI/meetings/wiki</p>"},{"location":"meetings/#physical-meetings","title":"Physical meetings","text":"<ul> <li>EESSI Community Meeting in Amsterdam (NL), 14-16 Sept 2022</li> </ul>"},{"location":"meetings/#physical-meetings-archive","title":"Physical meetings (archive)","text":""},{"location":"meetings/#2019","title":"2019","text":"<ul> <li>Meeting in Cambridge (UK), 20-21 May 2019</li> </ul>"},{"location":"meetings/#2020","title":"2020","text":"<ul> <li>Meeting in Groningen (NL), 16 Jan 2020</li> <li>Meeting in Delft (NL), 5 Mar 2020</li> </ul>"},{"location":"overview/","title":"Overview of the EESSI project","text":""},{"location":"overview/#scope-goals","title":"Scope &amp; Goals","text":"<p>Through the EESSI project, we want to set up a shared stack of scientific software installations, and by doing so avoid a lot of duplicate work across HPC sites.</p> <p>For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use.</p> <p>Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL, and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V).</p> <p>Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently.</p>"},{"location":"overview/#inspiration","title":"Inspiration","text":"<p>The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones.</p> <p>The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\".</p> <p>It has also been presented at the 5th EasyBuild User Meetings (slides, recorded talk), and is well documented.</p>"},{"location":"overview/#layered-structure","title":"Layered structure","text":"<p>The EESSI project consists of 3 layers.</p> <p></p> <p>The bottom layer is the filesystem layer, which is responsible for distributing the software stack across clients.</p> <p>The middle layer is a compatibility layer, which ensures that the software stack is compatible with multiple different client operating systems.</p> <p>The top layer is the software layer, which contains the actual scientific software applications and their dependencies.</p> <p>The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on.</p>"},{"location":"overview/#opportunities","title":"Opportunities","text":"<p>We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers.</p> <p>Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere.</p> <p>We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems.</p> <p>By working together with the developers of scientific software we can provide vetted installations for the broad HPC community.</p>"},{"location":"overview/#challenges","title":"Challenges","text":"<p>There are many challenges in an ambitious project like this, including (but probably not limited to):</p> <ul> <li>Finding time and manpower to get the software stack set up properly;</li> <li>Leveraging system sources like network interconnect (MPI &amp; co), accelerators (GPUs), ...;</li> <li>Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ...</li> <li>Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...;</li> <li>Integration with resource managers (Slurm) and vendor provided software (Cray PE);</li> <li>Convincing HPC site admins to adopt EESSI;</li> </ul>"},{"location":"overview/#current-status","title":"Current status","text":"<p>(June 2020)</p> <p>We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward.</p> <p>Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed.</p>"},{"location":"partners/","title":"Project partners","text":""},{"location":"partners/#delft-university-of-technology-the-netherlands","title":"Delft University of Technology (The Netherlands)","text":"<ul> <li>Robbert Eggermont</li> <li>Koen Mulderij</li> </ul>"},{"location":"partners/#dell-technologies-europe","title":"Dell Technologies (Europe)","text":"<ul> <li>Walther Blom, High Education &amp; Research</li> <li>Jaco van Dijk, Higher Education</li> </ul>"},{"location":"partners/#eindhoven-university-of-technology","title":"Eindhoven University of Technology","text":"<ul> <li>Patrick Van Brakel</li> </ul>"},{"location":"partners/#ghent-university-belgium","title":"Ghent University (Belgium)","text":"<ul> <li>Kenneth Hoste, HPC-UGent</li> </ul>"},{"location":"partners/#hpcnow-spain","title":"HPCNow! (Spain)","text":"<ul> <li>Oriol Mula Valls</li> </ul>"},{"location":"partners/#julich-supercomputing-centre-germany","title":"J\u00fclich Supercomputing Centre (Germany)","text":"<ul> <li>Alan O'Cais</li> </ul>"},{"location":"partners/#university-of-cambridge-united-kingdom","title":"University of Cambridge (United Kingdom)","text":"<ul> <li>Mark Sharpley, Research Computing Services Division</li> </ul>"},{"location":"partners/#university-of-groningen-the-netherlands","title":"University of Groningen (The Netherlands)","text":"<ul> <li>Bob Dr\u00f6ge, Center for Information Technology</li> <li>Henk-Jan Zilverberg, Center for Information Technology</li> </ul>"},{"location":"partners/#university-of-twente-the-netherlands","title":"University of Twente (The Netherlands)","text":"<ul> <li>Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS)</li> </ul>"},{"location":"partners/#university-of-oslo-norway","title":"University of Oslo (Norway)","text":"<ul> <li>Terje Kvernes</li> </ul>"},{"location":"partners/#university-of-bergen-norway","title":"University of Bergen (Norway)","text":"<ul> <li>Thomas R\u00f6blitz</li> </ul>"},{"location":"partners/#vrije-universiteit-amsterdam-the-netherlands","title":"Vrije Universiteit Amsterdam (The Netherlands)","text":"<ul> <li>Peter Stol</li> </ul>"},{"location":"partners/#surf-the-netherlands","title":"SURF (The Netherlands)","text":"<ul> <li>Caspar van Leeuwen</li> <li>Marco Verdicchio</li> <li>Bas van der Vlies</li> </ul>"},{"location":"pilot/","title":"Pilot repository","text":""},{"location":"pilot/#pilot-software-stack-202112","title":"Pilot software stack (2021.12)","text":""},{"location":"pilot/#caveats","title":"Caveats","text":"<p>The current EESSI pilot software stack (version 2021.12) is the 7th iteration, and there are some known issues and limitations, please take these into account:</p> <ul> <li>First of all: the EESSI pilot software stack is NOT READY FOR PRODUCTION!</li> </ul> <p>Do not use it for production work, and be careful when testing it on production systems!</p>"},{"location":"pilot/#reporting-problems","title":"Reporting problems","text":"<p>If you notice any problems, please report them via https://github.com/EESSI/software-layer/issues.</p>"},{"location":"pilot/#accessing-the-eessi-pilot-repository-through-singularity","title":"Accessing the EESSI pilot repository through Singularity","text":"<p>The easiest way to access the EESSI pilot repository is by using Singularity. If Singularity is installed already, no admin privileges are required. No other software is needed either on the host.</p> <p>A container image is available in the GitHub Container Registry  (see https://github.com/EESSI/filesystem-layer/pkgs/container/client-pilot). It only contains a minimal operating system + the necessary packages to access the EESSI pilot repository through CernVM-FS, and it is suitable for <code>aarch64</code>, <code>ppc64le</code>, and <code>x86_64</code>.</p> <p>The container image can be used directly by Singularity (no prior download required), as follows:</p> <ul> <li> <p>First, create some local directories in <code>/tmp/$USER</code> which will be bind mounted in the container:   <pre><code>mkdir -p /tmp/$USER/{var-lib-cvmfs,var-run-cvmfs,home}\n</code></pre>   These provides space for the CernVM-FS cache, and an empty home directory to use in the container.</p> </li> <li> <p>Set the <code>$SINGULARITY_BIND</code> and <code>$SINGULARITY_HOME</code> environment variables to configure Singularity:   <pre><code>export SINGULARITY_BIND=\"/tmp/$USER/var-run-cvmfs:/var/run/cvmfs,/tmp/$USER/var-lib-cvmfs:/var/lib/cvmfs\"\nexport SINGULARITY_HOME=\"/tmp/$USER/home:/home/$USER\"\n</code></pre></p> </li> <li> <p>Start the container using <code>singularity shell</code>, using <code>--fusemount</code> to mount the EESSI pilot repository   (using the <code>cvmfs2</code> command that is included in the container image):   <pre><code>export EESSI_PILOT=\"container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org\"\nsingularity shell --fusemount \"$EESSI_PILOT\" docker://ghcr.io/eessi/client-pilot:centos7\n</code></pre></p> </li> <li> <p>This should give you a shell in the container, where the EESSI pilot repository is mounted:    <pre><code>$ singularity shell --fusemount \"$EESSI_PILOT\" docker://ghcr.io/eessi/client-pilot:centos7\nINFO:    Using cached SIF image\nCernVM-FS: pre-mounted on file descriptor 3\nCernVM-FS: loading Fuse module... done\nSingularity&gt;\n</code></pre></p> </li> <li>It is possible that you see some scary looking warnings, but those can be ignored for now.</li> </ul> <p>To verify that things are working, check the contents of the <code>/cvmfs/pilot.eessi-hpc.org/versions/2021.12</code> directory:    <pre><code>Singularity&gt; ls /cvmfs/pilot.eessi-hpc.org/versions/2021.12\ncompat  init  software\n</code></pre></p>"},{"location":"pilot/#standard-installation","title":"Standard installation","text":"<p>For those with privileges on their system, there are a number of example installation scripts for different architectures and operating systems available in the EESSI demo repository.</p> <p>Here we prefer the Singularity approach as we can guarantee that the container image is up to date.</p>"},{"location":"pilot/#setting-up-the-eessi-environment","title":"Setting up the EESSI environment","text":"<p>Once you have the EESSI pilot repository mounted, you can set up the environment by sourcing the provided init script:</p> <pre><code>source /cvmfs/pilot.eessi-hpc.org/versions/2021.12/init/bash\n</code></pre> <p>If all goes well, you should see output like this:</p> <pre><code>Found EESSI pilot repo @ /cvmfs/pilot.eessi-hpc.org/versions/2021.12!\nUsing x86_64/intel/haswell as software subdirectory.\nUsing /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI pilot software stack, have fun!\n[EESSI pilot 2021.12] $ </code></pre> <p>Now you're all set up! Go ahead and explore the software stack using \"<code>module avail</code>\", and go wild with testing the available software installations!</p>"},{"location":"pilot/#testing-the-eessi-pilot-software-stack","title":"Testing the EESSI pilot software stack","text":"<p>Please test the EESSI pilot software stack as you see fit: running simple commands, performing small calculations or running small benchmarks, etc.</p> <p>Test scripts that have been verified to work correctly using the pilot software stack are available at https://github.com/EESSI/software-layer/tree/main/tests .</p>"},{"location":"pilot/#giving-feedback-or-reporting-problems","title":"Giving feedback or reporting problems","text":"<p>Any feedback is welcome, and questions or problems reports are welcome as well, through one of the EESSI communication channels:</p> <ul> <li>(preferred!) EESSI <code>software-layer</code> GitHub repository: https://github.com/EESSI/software-layer/issues</li> <li>EESSI mailing list (<code>eessi@list.rug.nl</code>)</li> <li>EESSI Slack: https://eessi-hpc.slack.com (get an invite via https://www.eessi-hpc.org/join)</li> <li>monthly EESSI meetings (first Thursday of the month at 2pm CEST)</li> </ul>"},{"location":"pilot/#available-software","title":"Available software","text":"<p>(last update: Mar 21st 2022)</p> <p>EESSI currently supports the following HPC applications as well as all their dependencies:</p> <ul> <li>GROMACS (2020.1 and 2020.4)</li> <li>OpenFOAM (v2006 and 8)</li> <li>R (4.0.0) + R-bundle-Bioconductor (3.11) + RStudio Server (1.3.1093)</li> <li>TensorFlow (2.3.1) and Horovod (0.21.3)</li> <li>OSU-Micro-Benchmarks (5.6.3)</li> <li>ReFrame (3.9.1)</li> <li>Spark (3.1.1)</li> <li>IPython (7.15.0)</li> <li>QuantumESPRESSO (6.6) (currently not available on <code>ppc64le</code>)</li> <li>WRF (3.9.1.1)</li> </ul> <pre><code>[EESSI pilot 2021.12] $ module --nx avail\n\n--------------------------- /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all ----------------------------\n   ant/1.10.8-Java-11                                              LMDB/0.9.24-GCCcore-9.3.0\n   Arrow/0.17.1-foss-2020a-Python-3.8.2                            lz4/1.9.2-GCCcore-9.3.0\n   Bazel/3.6.0-GCCcore-9.3.0                                       Mako/1.1.2-GCCcore-9.3.0\n   Bison/3.5.3-GCCcore-9.3.0                                       MariaDB-connector-c/3.1.7-GCCcore-9.3.0\n   Boost/1.72.0-gompi-2020a                                        matplotlib/3.2.1-foss-2020a-Python-3.8.2\n   cairo/1.16.0-GCCcore-9.3.0                                      Mesa/20.0.2-GCCcore-9.3.0\n   CGAL/4.14.3-gompi-2020a-Python-3.8.2                            Meson/0.55.1-GCCcore-9.3.0-Python-3.8.2\n   CMake/3.16.4-GCCcore-9.3.0                                      METIS/5.1.0-GCCcore-9.3.0\n   CMake/3.20.1-GCCcore-10.3.0                                     MPFR/4.0.2-GCCcore-9.3.0\n   code-server/3.7.3                                               NASM/2.14.02-GCCcore-9.3.0\n   DB/18.1.32-GCCcore-9.3.0                                        ncdf4/1.17-foss-2020a-R-4.0.0\n   DB/18.1.40-GCCcore-10.3.0                                       netCDF-Fortran/4.5.2-gompi-2020a\n   double-conversion/3.1.5-GCCcore-9.3.0                           netCDF/4.7.4-gompi-2020a\n   Doxygen/1.8.17-GCCcore-9.3.0                                    nettle/3.6-GCCcore-9.3.0\n   EasyBuild/4.5.0                                                 networkx/2.4-foss-2020a-Python-3.8.2\n   EasyBuild/4.5.1                                         (D)     Ninja/1.10.0-GCCcore-9.3.0\n   Eigen/3.3.7-GCCcore-9.3.0                                       NLopt/2.6.1-GCCcore-9.3.0\n   Eigen/3.3.9-GCCcore-10.3.0                                      NSPR/4.25-GCCcore-9.3.0\n   ELPA/2019.11.001-foss-2020a                                     NSS/3.51-GCCcore-9.3.0\n   expat/2.2.9-GCCcore-9.3.0                                       nsync/1.24.0-GCCcore-9.3.0\n   expat/2.2.9-GCCcore-10.3.0                                      numactl/2.0.13-GCCcore-9.3.0\n   FFmpeg/4.2.2-GCCcore-9.3.0                                      numactl/2.0.14-GCCcore-10.3.0\n   FFTW/3.3.8-gompi-2020a                                          OpenBLAS/0.3.9-GCC-9.3.0\n   FFTW/3.3.9-gompi-2021a                                          OpenBLAS/0.3.15-GCC-10.3.0\n   flatbuffers/1.12.0-GCCcore-9.3.0                                OpenFOAM/v2006-foss-2020a\n   FlexiBLAS/3.0.4-GCC-10.3.0                                      OpenFOAM/8-foss-2020a                              (D)\n   fontconfig/2.13.92-GCCcore-9.3.0                                OpenMPI/4.0.3-GCC-9.3.0\n   foss/2020a                                                      OpenMPI/4.1.1-GCC-10.3.0\n   foss/2021a                                                      OpenPGM/5.2.122-GCCcore-9.3.0\n   freetype/2.10.1-GCCcore-9.3.0                                   OpenSSL/1.1                                        (D)\n   FriBidi/1.0.9-GCCcore-9.3.0                                     OSU-Micro-Benchmarks/5.6.3-gompi-2020a\n   GCC/9.3.0                                                       Pango/1.44.7-GCCcore-9.3.0\n   GCC/10.3.0                                                      ParaView/5.8.0-foss-2020a-Python-3.8.2-mpi\n   GCCcore/9.3.0                                                   PCRE/8.44-GCCcore-9.3.0\n   GCCcore/10.3.0                                                  PCRE2/10.34-GCCcore-9.3.0\n   Ghostscript/9.52-GCCcore-9.3.0                                  Perl/5.30.2-GCCcore-9.3.0\n   giflib/5.2.1-GCCcore-9.3.0                                      Perl/5.32.1-GCCcore-10.3.0\n   git/2.23.0-GCCcore-9.3.0-nodocs                                 pixman/0.38.4-GCCcore-9.3.0\n   git/2.32.0-GCCcore-10.3.0-nodocs                        (D)     pkg-config/0.29.2-GCCcore-9.3.0\n   GLib/2.64.1-GCCcore-9.3.0                                       pkg-config/0.29.2-GCCcore-10.3.0\n   GLPK/4.65-GCCcore-9.3.0                                         pkg-config/0.29.2                                  (D)\n   GMP/6.2.0-GCCcore-9.3.0                                         pkgconfig/1.5.1-GCCcore-9.3.0-Python-3.8.2\n   GMP/6.2.1-GCCcore-10.3.0                                        PMIx/3.1.5-GCCcore-9.3.0\n   gnuplot/5.2.8-GCCcore-9.3.0                                     PMIx/3.2.3-GCCcore-10.3.0\n   GObject-Introspection/1.64.0-GCCcore-9.3.0-Python-3.8.2         poetry/1.0.9-GCCcore-9.3.0-Python-3.8.2\n   gompi/2020a                                                     protobuf-python/3.13.0-foss-2020a-Python-3.8.2\n   gompi/2021a                                                     protobuf/3.13.0-GCCcore-9.3.0\n   groff/1.22.4-GCCcore-9.3.0                                      pybind11/2.4.3-GCCcore-9.3.0-Python-3.8.2\n   groff/1.22.4-GCCcore-10.3.0                                     pybind11/2.6.2-GCCcore-10.3.0\n   GROMACS/2020.1-foss-2020a-Python-3.8.2                          Python/2.7.18-GCCcore-9.3.0\n   GROMACS/2020.4-foss-2020a-Python-3.8.2                  (D)     Python/3.8.2-GCCcore-9.3.0\n   GSL/2.6-GCC-9.3.0                                               Python/3.9.5-GCCcore-10.3.0-bare\n   gzip/1.10-GCCcore-9.3.0                                         Python/3.9.5-GCCcore-10.3.0\n   h5py/2.10.0-foss-2020a-Python-3.8.2                             PyYAML/5.3-GCCcore-9.3.0\n   HarfBuzz/2.6.4-GCCcore-9.3.0                                    Qt5/5.14.1-GCCcore-9.3.0\n   HDF5/1.10.6-gompi-2020a                                         QuantumESPRESSO/6.6-foss-2020a\n   Horovod/0.21.3-foss-2020a-TensorFlow-2.3.1-Python-3.8.2         R-bundle-Bioconductor/3.11-foss-2020a-R-4.0.0\n   hwloc/2.2.0-GCCcore-9.3.0                                       R/4.0.0-foss-2020a\n   hwloc/2.4.1-GCCcore-10.3.0                                      re2c/1.3-GCCcore-9.3.0\n   hypothesis/6.13.1-GCCcore-10.3.0                                RStudio-Server/1.3.1093-foss-2020a-Java-11-R-4.0.0\n   ICU/66.1-GCCcore-9.3.0                                          Rust/1.52.1-GCCcore-10.3.0\n   ImageMagick/7.0.10-1-GCCcore-9.3.0                              ScaLAPACK/2.1.0-gompi-2020a\n   IPython/7.15.0-foss-2020a-Python-3.8.2                          ScaLAPACK/2.1.0-gompi-2021a-fb\n   JasPer/2.0.14-GCCcore-9.3.0                                     scikit-build/0.10.0-foss-2020a-Python-3.8.2\n   Java/11.0.2                                             (11)    SciPy-bundle/2020.03-foss-2020a-Python-3.8.2\n   jbigkit/2.1-GCCcore-9.3.0                                       SciPy-bundle/2021.05-foss-2021a\n   JsonCpp/1.9.4-GCCcore-9.3.0                                     SCOTCH/6.0.9-gompi-2020a\n   LAME/3.100-GCCcore-9.3.0                                        snappy/1.1.8-GCCcore-9.3.0\n   libarchive/3.5.1-GCCcore-10.3.0                                 Spark/3.1.1-foss-2020a-Python-3.8.2\n   libcerf/1.13-GCCcore-9.3.0                                      SQLite/3.31.1-GCCcore-9.3.0\n   libdrm/2.4.100-GCCcore-9.3.0                                    SQLite/3.35.4-GCCcore-10.3.0\n   libevent/2.1.11-GCCcore-9.3.0                                   SWIG/4.0.1-GCCcore-9.3.0\n   libevent/2.1.12-GCCcore-10.3.0                                  Szip/2.1.1-GCCcore-9.3.0\n   libfabric/1.11.0-GCCcore-9.3.0                                  Tcl/8.6.10-GCCcore-9.3.0\n   libfabric/1.12.1-GCCcore-10.3.0                                 Tcl/8.6.11-GCCcore-10.3.0\n   libffi/3.3-GCCcore-9.3.0                                        tcsh/6.22.02-GCCcore-9.3.0\n   libffi/3.3-GCCcore-10.3.0                                       TensorFlow/2.3.1-foss-2020a-Python-3.8.2\n   libgd/2.3.0-GCCcore-9.3.0                                       time/1.9-GCCcore-9.3.0\n   libGLU/9.0.1-GCCcore-9.3.0                                      Tk/8.6.10-GCCcore-9.3.0\n   libglvnd/1.2.0-GCCcore-9.3.0                                    Tkinter/3.8.2-GCCcore-9.3.0\n   libiconv/1.16-GCCcore-9.3.0                                     UCX/1.8.0-GCCcore-9.3.0\n   libjpeg-turbo/2.0.4-GCCcore-9.3.0                               UCX/1.10.0-GCCcore-10.3.0\n   libpciaccess/0.16-GCCcore-9.3.0                                 UDUNITS/2.2.26-foss-2020a\n   libpciaccess/0.16-GCCcore-10.3.0                                UnZip/6.0-GCCcore-9.3.0\n   libpng/1.6.37-GCCcore-9.3.0                                     UnZip/6.0-GCCcore-10.3.0\n   libsndfile/1.0.28-GCCcore-9.3.0                                 WRF/3.9.1.1-foss-2020a-dmpar\n   libsodium/1.0.18-GCCcore-9.3.0                                  X11/20200222-GCCcore-9.3.0\n   LibTIFF/4.1.0-GCCcore-9.3.0                                     x264/20191217-GCCcore-9.3.0\n   libtirpc/1.2.6-GCCcore-9.3.0                                    x265/3.3-GCCcore-9.3.0\n   libunwind/1.3.1-GCCcore-9.3.0                                   xorg-macros/1.19.2-GCCcore-9.3.0\n   libxc/4.3.4-GCC-9.3.0                                           xorg-macros/1.19.3-GCCcore-10.3.0\n   libxml2/2.9.10-GCCcore-9.3.0                                    Xvfb/1.20.9-GCCcore-9.3.0\n   libxml2/2.9.10-GCCcore-10.3.0                                   Yasm/1.3.0-GCCcore-9.3.0\n   libyaml/0.2.2-GCCcore-9.3.0                                     ZeroMQ/4.3.2-GCCcore-9.3.0\n   LittleCMS/2.9-GCCcore-9.3.0                                     Zip/3.0-GCCcore-9.3.0\n   LLVM/9.0.1-GCCcore-9.3.0                                        zstd/1.4.4-GCCcore-9.3.0\n</code></pre>"},{"location":"pilot/#architecture-and-micro-architecture-support","title":"Architecture and micro-architecture support","text":""},{"location":"pilot/#x86_64","title":"x86_64","text":"<ul> <li>generic (currently implies <code>march=x86-64</code> and <code>-mtune=generic</code>)</li> <li>AMD<ul> <li>zen2 (Rome)</li> <li>zen3 (Milan)</li> </ul> </li> <li>Intel<ul> <li>haswell</li> <li>skylake_avx512</li> </ul> </li> </ul>"},{"location":"pilot/#aarch64arm64","title":"aarch64/arm64","text":"<ul> <li>generic (currently implies <code>-march=armv8-a</code> and <code>-mtune=generic</code>)</li> <li>AWS Graviton2</li> </ul>"},{"location":"pilot/#ppc64le","title":"ppc64le","text":"<ul> <li>generic</li> <li>power9le</li> </ul>"},{"location":"pilot/#easybuild-configuration","title":"EasyBuild configuration","text":"<p>EasyBuild v4.5.1 was used to install the software in the <code>2021.12</code> version of the pilot repository. For some installations pull requests with changes that will be included in later EasyBuild versions were leveraged, see the build script that was used.</p> <p>An example configuration of the build environment based on https://github.com/EESSI/software-layer can be seen here: <pre><code>$ eb --show-config\n#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath         (E) = /tmp/eessi-build/easybuild/build\ncontainerpath     (E) = /tmp/eessi-build/easybuild/containers\ndebug             (E) = True\nfilter-deps       (E) = Autoconf, Automake, Autotools, binutils, bzip2, cURL, DBus, flex, gettext, gperf, help2man, intltool, libreadline, libtool, Lua, M4, makeinfo, ncurses, util-linux, XZ, zlib\nfilter-env-vars   (E) = LD_LIBRARY_PATH\nhooks             (E) = /home/eessi-build/software-layer/eb_hooks.py\nignore-osdeps     (E) = True\ninstallpath       (E) = /cvmfs/pilot.eessi-hpc.org/2021.06/software/linux/x86_64/intel/haswell\nmodule-extensions (E) = True\npackagepath       (E) = /tmp/eessi-build/easybuild/packages\nprefix            (E) = /tmp/eessi-build/easybuild\nrepositorypath    (E) = /tmp/eessi-build/easybuild/ebfiles_repo\nrobot-paths       (D) = /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/software/EasyBuild/4.5.1/easybuild/easyconfigs\nrpath             (E) = True\nsourcepath        (E) = /tmp/eessi-build/easybuild/sources:\nsysroot           (E) = /cvmfs/pilot.eessi-hpc.org/versions/2021.12/compat/linux/x86_64\ntrace             (E) = True\nzip-logs          (E) = bzip2\n</code></pre></p>"},{"location":"software_layer/","title":"Software layer","text":"<p>The top layer of the EESSI project is the software layer, which provides the actual scientific software installations.</p> <p>To install the software we include in our stack, we use EasyBuild, a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation).</p> <p>To access these software installation we provide environment module files and use Lmod, a modern environment modules tool which has been widely adopted in the HPC community in recent years.</p> <p>We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture.</p> <p>The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.</p>"},{"location":"software_testing/","title":"Software testing","text":"<p>WARNING: development of the software test suite has only just started and is a work in progress. This page describes how the test suite will be designed, but many things are not implemented yet and the design may still change.</p>"},{"location":"software_testing/#description-of-the-software-test-suite","title":"Description of the software test suite","text":""},{"location":"software_testing/#framework","title":"Framework","text":"<p>The EESSI project uses the ReFrame framework for software testing. ReFrame is designed particularly for testing HPC software and thus has well integrated support for interacting with schedulers, as well as various launchers for MPI programs.</p>"},{"location":"software_testing/#test-variants","title":"Test variants","text":"<p>The EESSI software stack can be used in various ways, e.g. by using the container or when the CVMFS software stack is mounted natively. This means the commands that need to be run to test an application are different in both cases. Similarly, systems may have different hardware (CPUs v.s. GPUs, system size, etc). Thus, tests - e.g. a GROMACS test - may have different variants: one designed to run on CPUs, one on GPUs, one designed to run through the container, etc.</p> <p>The main goal of the EESSI test suite is to test the software stack on systems that have the EESSI CVMFS mounted natively. Some tests may also have variants that can run the same test through the container, but note that this setup is technically much more difficult. Thus, the main focus is on tests that run with a native CVMFS mount of the EESSI stack.</p> <p>By default, ReFrame runs all test variants it find. Thus, in our test suite, we prespecify a number of tags that can be used to select an appropriate subset of tests for your system. We recognize the following tags:</p> <ul> <li>container: tests that use the EESSI container to run the software. E.g. one variant of our GROMACS test uses <code>singularity exec</code> to launch the EESSI container, load the GROMACS module, and run the GROMACS test. </li> <li><code>native</code>: tests that rely on the EESSI software stack being available through the modules system. E.g. one variant of the GROMACS test loads the GROMACS module and runs the GROMACS test.</li> <li><code>singlecore</code>: tests designed to run on a single core</li> <li><code>singlenode</code>: tests designed to run on a single (multicore) node (note: may still use MPI for multiprocessing)</li> <li><code>small</code>: tests designed to run on 2-8 nodes.</li> <li><code>large</code>: tests designed to run on &gt;9 nodes.</li> <li><code>cpu</code>: test designed to run on CPU.</li> <li><code>gpu</code>, gpu_nvidia, gpu_amd: test designed to run on GPUs / nvidia GPUs / AMD GPUs.</li> </ul>"},{"location":"software_testing/#how-to-run-the-test-suite","title":"How to run the test suite","text":""},{"location":"software_testing/#general-requirements","title":"General requirements","text":"<ul> <li>A copy of the <code>tests</code> directory from software repository</li> </ul>"},{"location":"software_testing/#requirements-for-container-based-tests","title":"Requirements for container-based tests","text":"<p>Specifically for container-based tests, there are some requirements on the host system:</p> <ul> <li>An installation of ReFrame</li> <li>An MPI installation (to launch MPI tests) or PMIx-based launcher (e.g. SLURM compiled with PMIx support)</li> <li>Singularity</li> </ul> <p>The container based tests will use a so-called shared alien CVMFS cache to store temporary data. In addition, they use a local CVMFS cache for speed. For this reason, the container tests need to be pointed to one directory that is shared between nodes on your system, and one directory that is node-specific (preferably a local disk). The <code>shared_alien_cache_minimal.sh</code> script that is part of the test suite defines these, and sets up the correct CVMFS configuration. You will have to adapt the <code>SHAREDSPACE</code> and <code>LOCALSPACE</code> variables in that script for your system, and point them to a shared and node-local directory.</p>"},{"location":"software_testing/#setting-up-a-reframe-configuration-file","title":"Setting up a ReFrame configuration file","text":"<p>Once the prerequisites have been met, you'll need to create a ReFrame configuration file that matches your system (see the ReFrame documentation). If you want to use the container-based tests, you have to define a partition programming environment called <code>container</code> and make sure it loads any modules needed to provide the MPI installation and singularity command. For an example configuration file, check the <code>tests/reframe/config/settings.py</code> in the software-layer repository. Other than (potential) adaptations to the <code>container</code> environment, you should only really need to change the <code>systems</code> part.</p>"},{"location":"software_testing/#adapting-the-tests-to-your-system","title":"Adapting the tests to your system","text":"<p>For now, you will have to adapt the number of tasks specified in full-node tests to match the number of cores your machine has in a single node (in the future, you should be able to do this through the reframe configuration file). To do so, change all <code>self.num_tasks_per_node</code> you find in the various tests to that core count (unless they are 1, in which case the test specifically intended for only 1 process per node).</p>"},{"location":"software_testing/#an-example-run","title":"An example run","text":"<p>In this example, we assume your current directory is the <code>tests/reframe</code> folder. To list e.g. all single node, cpu-based application tests on a system that has the EESSI software environment available natively, you execute: <pre><code>reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu\n</code></pre> (assuming you adapted the config file in <code>config/settings.py</code> for your system). This should list the tests that are selected based on the provided tags. To run the tests, change the <code>-l</code> argument into a <code>-r</code>: <pre><code>reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu --performance-report\n</code></pre> To run the same tests with using the EESSI container, run: <pre><code>reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t container -t single -t cpu --performance-report\n</code></pre> Note that not all tests necessarily have implementations to run using the EESSI container: the primary focus of the test suite is for HPC sites to check the performance of their software suite. Such sites should have CVMFS mounted natively for optimal performance anyway.</p>"},{"location":"filesystem_layer/stratum1/","title":"Setting up a Stratum 1","text":"<p>Setting up a Stratum 1 involves the following steps:</p> <ul> <li>set up the Stratum 1, preferably by running the Ansible playbook that we provide;</li> <li>request a Stratum 0 firewall exception for your Stratum 1 server;</li> <li>request a <code>&lt;your site&gt;.stratum1.cvmfs.eessi-infra.org</code> DNS entry;</li> <li>open a pull request to include the URL to your Stratum 1 in the EESSI configuration.</li> </ul> <p>The last two steps can be skipped if you want to host a \"private\" Stratum 1 for your site.</p>"},{"location":"filesystem_layer/stratum1/#requirements-for-a-stratum-1","title":"Requirements for a Stratum 1","text":"<p>The main requirements for a Stratum 1 server are a good network connection to the clients it is going to serve, and sufficient disk space. For the EESSI pilot, a few hundred gigabytes should suffice, but for production environments at least 1 TB would be recommended.</p> <p>In terms of cores and memory, a machine with just a few (~4) cores and 4-8 GB of memory should suffice.</p> <p>Various Linux distributions are supported, but we recommend one based on RHEL 7 or 8.</p> <p>Finally, make sure that ports 80 (for the Apache web server) and 8000 are open.</p>"},{"location":"filesystem_layer/stratum1/#step-1-set-up-the-stratum-1","title":"Step 1: set up the Stratum 1","text":"<p>The recommended way for setting up an EESSI Stratum 1 is by running the Ansible playbook <code>stratum1.yml</code> from the filesystem-layer repository on GitHub.</p> <p>Installing a Stratum 1 requires a GEO API license key, which will be used to find the (geographically) closest Stratum 1 server for your client and proxies. More information on how to (freely) obtain this key is available in the CVMFS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-replica.html#geo-api-setup.</p> <p>You can put your license key in the local configuration file <code>inventory/local_site_specific_vars.yml</code>.</p> <p>Furthermore, the Stratum 1 runs a Squid server. The template configuration file can be found at <code>templates/eessi_stratum1_squid.conf.j2</code>. If you want to customize it, for instance for limiting the access to the Stratum 1, you can make your own version of this template file  and point to it by setting <code>local_stratum1_cvmfs_squid_conf_src</code> in <code>inventory/local_site_specific_vars.yml</code>. See the comments in the example file for more details.</p> <p>Start by installing Ansible:</p> <pre><code>sudo yum install -y ansible\n</code></pre> <p>Then install Ansible roles for EESSI:</p> <pre><code>ansible-galaxy role install -r requirements.yml -p ./roles --force\n</code></pre> <p>Make sure you have enough space in <code>/srv</code> (on the Stratum 1) since the snapshot of the Stratum 0 will end up there by default. To alter the directory where the snapshot gets copied to you can add this variable in <code>inventory/host_vars/&lt;url-or-ip-to-your-stratum1&gt;</code>:</p> <pre><code>cvmfs_srv_mount: /srv\n</code></pre> <p>Make sure that you have added the hostname or IP address of your server to the <code>inventory/hosts</code> file. Finally, install the Stratum 1 using one of the two following options.</p> <p>Option 1:</p> <pre><code># -b to run as root, optionally use -K if a sudo password is required\nansible-playbook -b [-K] -e @inventory/local_site_specific_vars.yml stratum1.yml\n</code></pre> <p>Option2:</p> <p>Create a ssh key pair and make sure the <code>ansible-host-keys.pub</code> is in the <code>$HOME/.ssh/authorized_keys</code> file on your Stratum 1 server.</p> <pre><code>ssh-keygen -b 2048 -t rsa -f ~/.ssh/ansible-host-keys -q -N \"\"\n</code></pre> <p>Then run the playbook:</p> <pre><code>ansible-playbook -b --private-key ~/.ssh/ansible-host-keys -e @inventory/local_site_specific_vars.yml stratum1.yml\n</code></pre> <p>Running the playbook will automatically make replicas of all the repositories defined in <code>group_vars/all.yml</code>.</p>"},{"location":"filesystem_layer/stratum1/#step-2-request-a-firewall-exception","title":"Step 2: request a firewall exception","text":"<p>(This step is not implemented yet and can be skipped)</p> <p>You can request a firewall exception rule to be added for your Stratum 1 server by opening an issue on the GitHub page of the filesystem layer repository.</p> <p>Make sure to include the IP address of your server.</p>"},{"location":"filesystem_layer/stratum1/#step-3-verification-of-the-stratum-1","title":"Step 3: Verification of the Stratum 1","text":"<p>When the playbook has finished your Stratum 1 should be ready. In order to test your Stratum 1, even without a client installed, you can use <code>curl</code>.</p> <p><pre><code>curl --head http://&lt;url-or-ip-to-your-stratum1&gt;/cvmfs/pilot.eessi-hpc.org/.cvmfspublished\n</code></pre> This should return:</p> <pre><code>HTTP/1.1 200 OK\n...\nX-Cache: MISS from &lt;url-or-ip-to-your-stratum1&gt;\n</code></pre> <p>The second time you run it, you should get a cache hit:</p> <pre><code>X-Cache: HIT from &lt;url-or-ip-to-your-stratum1&gt;\n</code></pre> <p>Example with the Norwegian Stratum 1:</p> <pre><code>curl --head http://bgo-no.stratum1.cvmfs.eessi-infra.org/cvmfs/pilot.eessi-hpc.org/.cvmfspublished\n</code></pre> <p>You can also test access to your Stratum 1 from a client, for which you will have to install the CVMFS client. </p> <p>Then run the following command to add your newly created Stratum 1 to the existing list of EESSI Stratum 1 servers by creating a local CVMFS configuration file:</p> <pre><code>echo 'CVMFS_SERVER_URL=\"http://&lt;url-or-ip-to-your-stratum1&gt;/cvmfs/@fqrn@;$CVMFS_SERVER_URL\"' | sudo tee -a /etc/cvmfs/domain.d/eessi-hpc.org.local\n</code></pre> <p>If this is the first time you set up the client you now run:</p> <pre><code>sudo cvmfs_config setup\n</code></pre> <p>If you already had configured the client before, you can simply reload the config:</p> <pre><code>sudo cvmfs_config reload -c pilot.eessi-hpc.org\n</code></pre> <p>Finally, verify that the client connects to your new Stratum 1 by running:</p> <pre><code>cvmfs_config stat -v pilot.eessi-hpc.org\n</code></pre> <p>Assuming that your new Stratum 1 is the geographically closest one to your client, this should return:</p> <pre><code>Connection: http://&lt;url-or-ip-to-your-stratum1&gt;/cvmfs/pilot.eessi-hpc.org through proxy DIRECT (online)\n</code></pre>"},{"location":"filesystem_layer/stratum1/#step-4-request-an-eessi-dns-name","title":"Step 4: request an EESSI DNS name","text":"<p>In order to keep the configuration clean and easy, all the EESSI Stratum 1 servers have a DNS name <code>&lt;your site&gt;.stratum1.cvmfs.eessi-infra.org</code>, where <code>&lt;your site&gt;</code> is often a short name or abbreviation followed by the country code (e.g. <code>rug-nl</code> or <code>bgo-no</code>).  You can request this for your Stratum 1 by mentioning this in the issue that you created in Step 2, or by opening another issue.</p>"},{"location":"filesystem_layer/stratum1/#step-5-include-your-stratum-1-in-the-eessi-configuration","title":"Step 5: include your Stratum 1 in the EESSI configuration","text":"<p>If you want to include your Stratum 1 in the EESSI configuration, i.e. allow any (nearby) client to be able to use it, you can open a pull request with updated configuration files. You will only have to add the URL to your Stratum 1 to the  <code>urls</code> list of the <code>eessi_cvmfs_server_urls</code> variable in the <code>all.yml</code> file.</p>"},{"location":"meetings/2022-09-amsterdam/","title":"EESSI Community Meeting (Sept'22, Amsterdam)","text":""},{"location":"meetings/2022-09-amsterdam/#practical-info","title":"Practical info","text":"<ul> <li>dates: Wed-Fri 14-16 Sept'22</li> <li>in conjunction with CernVM workshop @ Nikhef (Mon-Tue 12-13 Sept'22)</li> <li>venue: \"Polderzaal\" at Cafe-Restaurant Polder (Google   Maps), sponsored by SURF</li> <li>registration (closed since Fri 9 Sept'22)</li> <li>Slack channel: <code>community-meeting-2022</code> in EESSI Slack</li> <li>YouTube playlist with recorded talks</li> </ul>"},{"location":"meetings/2022-09-amsterdam/#agenda","title":"Agenda","text":"<p>(subject to changes)</p> <p>We envision a mix of presentations, experience reports, demos, and hands-on sessions and/or hackathons related to the EESSI project.</p> <p>If you would like to give a talk or host a session, please let us know via the EESSI Slack!</p>"},{"location":"meetings/2022-09-amsterdam/#wed-14-sept-2022","title":"Wed 14 Sept 2022","text":"<ul> <li>[10:00-13:00] Welcome session<ul> <li>[10:00-10:30] Walk-in, coffee</li> <li>[10:30-12:00] Round table discussion (not live-streamed!)</li> </ul> </li> <li>[12:00-13:00] Lunch</li> <li>[13:00-15:00] Presentations on EESSI<ul> <li>[13:00-13:30] Introduction to EESSI (Caspar) [slides - recording]</li> <li>[13:30-14:00] Hands-on: how to use EESSI (Kenneth) [slides - recording]</li> <li>[14:00-14:30] EESSI use cases (Kenneth) [(slides - recording]</li> <li>[14:30-15:00] EESSI for sysadmins (Thomas) [slides - recording]</li> </ul> </li> <li>[15:00-15:30] Coffee break</li> <li>[15:30-17:00] Presentations on EESSI (continued)<ul> <li>[15:30-16:00] Hands-on: installing EESSI (Thomas/Kenneth)</li> <li>[16:00-16:45] ComputeCanada site talk (Bart Oldeman, remote) [slides - recording]</li> <li>[16:45-17:15] Magic Castle (Felix-Antoine Fortin, remote) [slides - recording]</li> </ul> </li> <li>[19:00-...] Group dinner @ Saravanaa Bhavan (sponsored by Dell Technologies)<ul> <li>address: Stadhouderskade 123-124, Amsterdam</li> </ul> </li> </ul>"},{"location":"meetings/2022-09-amsterdam/#thu-15-sept-2022","title":"Thu 15 Sept 2022","text":"<ul> <li>[09:30-12:00] More focused presentations on aspects of EESSI<ul> <li>[09:30-10:00] EESSI behind the scenes: compat layer (Bob) [slides - recording]</li> <li>[10:00-10:30] EESSI behind the scenes: software layer (Kenneth) [slides - recording]</li> <li>[10:30-11:00] Coffee break</li> <li>[11:00-11:30] EESSI behind the scenes: infrastructure (Terje) [slides - recording]</li> <li>[11:30-12:00] Status on RISC-V support (Kenneth) [slides - recording]</li> </ul> </li> <li>[12:00-13:00] Lunch</li> <li>[13:00-14:00] Discussions/hands-on sessions/hackathon</li> <li>[14:00-14:30] Status on GPU support (Alan) [slides - recording]</li> <li>[14:30-15:00] Status on build-and-deploy bot (Thomas) [slides - recording]</li> <li>[15:00-15:30] Coffee break</li> <li>[15:30-17:00] Discussions/hands-on sessions/hackathon (continued)<ul> <li>Hands-on with GPUs (Alan)</li> <li>Hands-on with bot (Thomas/Kenneth)</li> </ul> </li> <li>[19:00-...] Group dinner @ Italia Oggi (sponsored by HPC-UGent)<ul> <li>address: Binnen Bantammerstraat 11, Amsterdam</li> </ul> </li> </ul>"},{"location":"meetings/2022-09-amsterdam/#fri-16-sept-2022","title":"Fri 16 Sept 2022","text":"<ul> <li>[09:30-12:00] Presentations on future work<ul> <li>[09:30-10:00] Testing in software layer (Caspar) [slides - recording]</li> <li>[10:00-10:30] MultiXscale project (Alan) [slides - recording]</li> <li>[10:30-11:00] Coffee break</li> <li>[11:00-11:30] Short-term future work (Kenneth) [slides - recording]</li> </ul> </li> <li>[11:30-12:00] Discussion: future management structure of EESSI (Alan) [slides - recording]</li> <li>[12:00-13:00] Lunch</li> <li>[13:00-14:00] Site reports [recording]<ul> <li>NESSI (Thomas) [slides]</li> <li>NLPL (Stephan) [slides]</li> <li>HPCNow! (Danilo) [slides]</li> <li>Azure (Hugo) [slides]</li> </ul> </li> <li>[14:00-14:30] Discussion: what would make or break EESSI for your site? (notes - recording)</li> <li>[14:30-15:45] Discussions/hands-on sessions/hackathon<ul> <li>Hands-on with GPU support (Alan)</li> <li>Hands-on with bot (Thomas/Kenneth)</li> <li>Hands-on with software testing (Caspar)</li> </ul> </li> <li>We need to leave the room by 16:00!</li> </ul>"},{"location":"software_layer/build_nodes/","title":"Build nodes","text":"<p>Any system can be used as a build node to create additional software installations that should be added to the EESSI CernVM-FS repository.</p>"},{"location":"software_layer/build_nodes/#requirements","title":"Requirements","text":"<p>OS and software:</p> <ul> <li>GNU/Linux (any distribution) as operating system;</li> <li>a recent version of Singularity (&gt;= 3.6 is recommended);<ul> <li>check with <code>singularity --version</code></li> </ul> </li> <li><code>screen</code> or <code>tmux</code> is highly recommended;</li> </ul> <p>Admin privileges are not required, as long as Singularity is installed.</p> <p>Resources:</p> <ul> <li>8 or more cores is recommended (though not strictly required);</li> <li>at least 50GB of free space on a local filesystem (like <code>/tmp</code>);</li> <li>at least 16GB of memory (2GB/core or higher recommended);</li> </ul> <p>Instructions to install Singularity and screen (click to show commands):</p> CentOS 8 (<code>x86_64</code> or <code>aarch64</code> or <code>ppc64le</code>) <pre><code>sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm\nsudo dnf update -y\nsudo dnf install -y screen singularity\n</code></pre>"},{"location":"software_layer/build_nodes/#setting-up-the-container","title":"Setting up the container","text":"<p>Warning</p> <p>It is highly recommended to start a <code>screen</code> or <code>tmux</code> session first!</p> <p>A container image is provided that includes everything that is required to set up a writable overlay on top of the EESSI CernVM-FS repository.</p> <p>First, pick a location on a local filesystem for the temporary directory:</p> <p>Requirements:</p> <ul> <li>Do not use a shared filesystem like NFS, Lustre or GPFS.</li> <li>There should be at least 50GB of free disk space in this local filesystem (more is better).</li> <li>There should be no automatic cleanup of old files via a cron job on this local filesystem.</li> <li>Try to make sure the directory is unique (not used by anything else).</li> </ul> <p>NB. If you are going to install on a separate drive (due to lack of space on /), then you need to set some variables to  point to that location. You will also need to bind mount it in the <code>singularity</code> command. Let's say that you drive is  mounted in /srt. Then you change the relevant commands below to this: <pre><code>export EESSI_TMPDIR=/srt/$USER/EESSI\nmkdir -p $EESSI_TMPDIR\nmkdir /srt/tmp\nexport SINGULARITY_BIND=\"$EESSI_TMPDIR/var-run-cvmfs:/var/run/cvmfs,$EESSI_TMPDIR/var-lib-cvmfs:/var/lib/cvmfs,/srt/tmp:/tmp\"\nsingularity shell -B /srt --fusemount \"$EESSI_PILOT_READONLY\" --fusemount \"$EESSI_PILOT_WRITABLE_OVERLAY\" docker://ghcr.io/eessi/build-node:debian10\n</code></pre></p> <p>We will assume that <code>/tmp/$USER/EESSI</code> meets these requirements:</p> <pre><code>export EESSI_TMPDIR=/tmp/$USER/EESSI\nmkdir -p $EESSI_TMPDIR\n</code></pre> <p>Create some subdirectories in this temporary directory:</p> <pre><code>mkdir -p $EESSI_TMPDIR/{home,overlay-upper,overlay-work}\nmkdir -p $EESSI_TMPDIR/{var-lib-cvmfs,var-run-cvmfs}\n</code></pre> <p>Configure Singularity cache directory, bind mounts, and (fake) home directory:</p> <pre><code>export SINGULARITY_CACHEDIR=$EESSI_TMPDIR/singularity_cache\nexport SINGULARITY_BIND=\"$EESSI_TMPDIR/var-run-cvmfs:/var/run/cvmfs,$EESSI_TMPDIR/var-lib-cvmfs:/var/lib/cvmfs\"\nexport SINGULARITY_HOME=\"$EESSI_TMPDIR/home:/home/$USER\"\n</code></pre> <p>Define values to pass to <code>--fusemount` in</code>singularity`` command:</p> <pre><code>export EESSI_PILOT_READONLY=\"container:cvmfs2 pilot.eessi-hpc.org /cvmfs_ro/pilot.eessi-hpc.org\"\nexport EESSI_PILOT_WRITABLE_OVERLAY=\"container:fuse-overlayfs -o lowerdir=/cvmfs_ro/pilot.eessi-hpc.org -o upperdir=$EESSI_TMPDIR/overlay-upper -o workdir=$EESSI_TMPDIR/overlay-work /cvmfs/pilot.eessi-hpc.org\"\n</code></pre> <p>Start the container (which includes Debian 10, CernVM-FS and fuse-overlayfs):</p> <pre><code>singularity shell --fusemount \"$EESSI_PILOT_READONLY\" --fusemount \"$EESSI_PILOT_WRITABLE_OVERLAY\" docker://ghcr.io/eessi/build-node:debian10\n</code></pre> <p>Once the container image has been downloaded and converted to a Singularity image (SIF format), you should get a prompt like this:</p> <pre><code>...\nCernVM-FS: loading Fuse module... done\n\nSingularity&gt;\n</code></pre> <p>and the EESSI CernVM-FS repository should be mounted:</p> <pre><code>Singularity&gt; ls /cvmfs/pilot.eessi-hpc.org\n2020.12  2021.03  latest\n</code></pre>"},{"location":"software_layer/build_nodes/#setting-up-the-environment","title":"Setting up the environment","text":"<p>Set up the environment by starting a Gentoo Prefix session using the <code>startprefix</code> command.</p> <p>Make sure you use the correct version of the EESSI pilot repository!</p> <pre><code>export EESSI_PILOT_VERSION='2021.03'\n/cvmfs/pilot.eessi-hpc.org/${EESSI_PILOT_VERSION}/compat/linux/$(uname -m)/startprefix\n</code></pre>"},{"location":"software_layer/build_nodes/#installing-software","title":"Installing software","text":"<p>Clone the software-layer repository:</p> <pre><code>git clone https://github.com/EESSI/software-layer.git\n</code></pre> <p>Run the software installation script in <code>software-layer</code>:</p> <pre><code>cd software-layer\n./EESSI-pilot-install-software.sh\n</code></pre> <p>This script will figure out the CPU microarchitecture of the host automatically (like <code>x86_64/intel/haswell</code>).</p> <p>To build generic software installations (like <code>x86_64/generic</code>), use the <code>--generic</code> option:</p> <pre><code>./EESSI-pilot-install-software.sh --generic\n</code></pre> <p>Once all missing software has been installed, you should see a message like this:</p> <pre><code>No missing modules!\n</code></pre>"},{"location":"software_layer/build_nodes/#creating-tarball-to-ingest","title":"Creating tarball to ingest","text":"<p>Before tearing down the build node, you should create tarball to ingest into the EESSI CernVM-FS repository.</p> <p>To create a tarball of all installations, assuming your build host is <code>x86_64/intel/haswell</code>:</p> <pre><code>export EESSI_PILOT_VERSION='2021.03'\ncd /cvmfs/pilot.eessi-hpc.org/${EESSI_PILOT_VERSION}/software/linux\neessi_tar_gz=\"$HOME/eessi-${EESSI_PILOT_VERSION}-haswell.tar.gz\"\ntar cvfz ${eessi_tar_gz} x86_64/intel/haswell\n</code></pre> <p>To create a tarball for specific installations, make sure you pick up both the software installation directories and the corresponding module files:</p> <pre><code>eessi_tar_gz=\"$HOME/eessi-${EESSI_PILOT_VERSION}-haswell-OpenFOAM.tar.gz\"\ntar cvfz ${eessi_tar_gz} x86_64/intel/haswell/software/OpenFOAM modules/all//OpenFOAM\n</code></pre> <p>This tarball should be uploaded to the Stratum 0 server for ingestion. If needed, you can ask for help in the EESSI <code>#software-layer</code> Slack channel</p>"}]}